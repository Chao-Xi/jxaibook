# 0 OpenAI Responses API ä¸ Agents SDK

## 1 What Is the Responses API?


Responses API æ˜¯ OpenAI æ¨å‡ºçš„æ–°ä¸€ä»£æ ¸å¿ƒ APIï¼Œæ—¨åœ¨ç»Ÿä¸€å¹¶å‡çº§ç°æœ‰çš„å¤šç§æ¥å£èƒ½åŠ›ï¼Œä¸ºæ„å»ºç°ä»£åŒ–ã€å…·å¤‡è¡ŒåŠ¨èƒ½åŠ›ï¼ˆagenticï¼‰çš„ AI åº”ç”¨æä¾›åŸºç¡€è®¾æ–½ã€‚

åœ¨æ—©æœŸé˜¶æ®µï¼ŒOpenAI å…ˆåæ¨å‡ºäº† **Completions API**ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰ã€**Chat Completions API**ï¼ˆå¯¹è¯äº¤äº’ï¼‰ä»¥åŠ **Assistants API**ï¼ˆå·¥å…·è°ƒç”¨ä¸ä»£ç†èƒ½åŠ›ï¼‰ã€‚è¿™äº› API å„è‡ªæœåŠ¡äºä¸åŒåœºæ™¯ï¼Œä½†ä¹Ÿç»™å¼€å‘è€…å¸¦æ¥äº†é€‰æ‹©æˆæœ¬å’Œæ¶æ„å¤æ‚æ€§ã€‚

Responses API çš„æ ¸å¿ƒç›®æ ‡æ˜¯ **â€œä¸€ä½“åŒ–â€** â€”â€” å°†æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ã€å¤šè½®çŠ¶æ€ç®¡ç†ã€å·¥å…·è°ƒç”¨ï¼ˆå¦‚ç½‘é¡µæœç´¢ã€æ–‡ä»¶è®¿é—®ã€ä»£ç æ‰§è¡Œï¼‰ä»¥åŠä»£ç†å¼å·¥ä½œæµç»Ÿä¸€åˆ°ä¸€ä¸ª API ä¸­ã€‚**å®ƒä¸ä»…ç»§æ‰¿äº† Chat Completions çš„æ˜“ç”¨æ€§ï¼Œè¿˜æ•´åˆäº† Assistants API çš„é«˜çº§èƒ½åŠ›ï¼Œå¹¶è¢«å®˜æ–¹å®šä½ä¸ºåè€…çš„æ›¿ä»£æ–¹æ¡ˆ**ã€‚

æ ¹æ® OpenAI å…¬å‘Šï¼ŒAssistants API è®¡åˆ’äº 2026 å¹´é€æ­¥å¼ƒç”¨ï¼Œå¼€å‘è€…åº”å¼€å§‹å‘ Responses API è¿ç§»ã€‚

ä¸ Chat Completions API ç›¸æ¯”ï¼ŒResponses API åœ¨æ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸Šå¹¶æ— å·®å¼‚ï¼Œä½†åœ¨åŠŸèƒ½å±‚é¢æ˜¾è‘—å¢å¼ºï¼šå®ƒåŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨ã€å†…å»ºçŠ¶æ€ç®¡ç†ï¼ˆæ— éœ€æ‰‹åŠ¨ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰ã€æ”¯æŒ MCPï¼ˆModel Context Protocolï¼‰ï¼Œå¹¶å¯ç”¨äºæ„å»ºå¤šæ­¥éª¤å†³ç­–ä¸æ‰§è¡Œçš„æ™ºèƒ½ä½“å·¥ä½œæµã€‚

å› æ­¤ï¼ŒResponses API ç‰¹åˆ«é€‚åˆéœ€è¦ **æµè§ˆç½‘é¡µã€è§£ææ–‡ä»¶ã€æ‰§è¡Œä»£ç ã€è‡ªåŠ¨è§„åˆ’ä¸å†³ç­–** çš„åº”ç”¨åœºæ™¯ï¼Œä¹Ÿè¢«è§†ä¸º OpenAI å¹³å°æœªæ¥ Agent æ¶æ„çš„æ ¸å¿ƒåŸºçŸ³ã€‚

## äºŒ Chat Completions API ä¸ Responses API æŠ€æœ¯å¯¹æ¯”è¡¨


| å¯¹æ¯”ç»´åº¦                   | Chat Completions API | Responses API                                            |
| ---------------------- | -------------------- | -------------------------------------------------------- |
| **API å®šä½**             | å¯¹è¯å¼æ–‡æœ¬ç”Ÿæˆæ¥å£            | ç»Ÿä¸€çš„æ™ºèƒ½ä½“ï¼ˆAgentï¼‰æ ¸å¿ƒ API                                      |
| **è®¾è®¡ç›®æ ‡**               | ç®€å•å¯¹è¯ä¸æ–‡æœ¬ç”Ÿæˆ            | é¢å‘è¡ŒåŠ¨çš„ AI åº”ç”¨ä¸ Agent å·¥ä½œæµ                                   |
| **æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›**             | âœ… é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆ            | âœ… åŒç­‰é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆ                                              |
| **å¤šè½®å¯¹è¯æ”¯æŒ**             | âœ… æ”¯æŒ                 | âœ… æ”¯æŒ                                                     |
| **çŠ¶æ€ç®¡ç†**               | âŒ éœ€å¼€å‘è€…æ‰‹åŠ¨ç»´æŠ¤ä¸Šä¸‹æ–‡        | âœ… å†…å»ºçŠ¶æ€ç®¡ç†ï¼ˆResponse / Conversation IDï¼‰                     |
| **å·¥å…·è°ƒç”¨ï¼ˆToolsï¼‰**        | âŒ ä¸æ”¯æŒ                | âœ… åŸç”Ÿæ”¯æŒ                                                   |
| **å†…å»ºå·¥å…·ç±»å‹**             | âŒ æ—                   | âœ… Web Search / File Read / Code Execution / Computer Use |
| **æ–‡ä»¶å¤„ç†èƒ½åŠ›**             | âŒ ä¸æ”¯æŒ                | âœ… æ”¯æŒè¯»å–ä¸è§£ææ–‡ä»¶ï¼ˆå¦‚ PDFï¼‰                                       |
| **ä»£ç æ‰§è¡Œèƒ½åŠ›**             | âŒ ä¸æ”¯æŒ                | âœ… å†…å»º Code Interpreter                                    |
| **Agentic Workflow**   | âŒ ä¸é€‚åˆ                | âœ… åŸç”Ÿæ”¯æŒå¤šæ­¥éª¤å†³ç­–ä¸æ‰§è¡Œ                                           |
| **è§„åˆ’ä¸æ¨ç†èƒ½åŠ›**            | âš ï¸ éœ€è‡ªè¡Œå°è£…             | âœ… å†…å»º Reason â†’ Plan â†’ Act â†’ Observe æ¨¡å¼                    |
| **MCP æ”¯æŒ**             | âŒ ä¸æ”¯æŒ                | âœ… æ”¯æŒ Model Context Protocol                              |
| **è¾“å‡ºç»“æ„**               | ä»¥ `message` ä¸ºä¸»       | å¤šç±»å‹è¾“å‡ºï¼ˆtext / tool-call / search-call ç­‰ï¼‰                  |
| **å¼•ç”¨ / æ³¨è§£ï¼ˆCitationsï¼‰** | âŒ æ—                   | âœ… æ”¯æŒ annotationsï¼ˆå¦‚æœç´¢å¼•ç”¨ï¼‰                                  |
| **æ¨¡å‹å¯ç”¨æ€§**              | æ–‡æœ¬æ¨¡å‹ä¸ºä¸»               | éƒ¨åˆ†å¸¦å·¥å…·æ¨¡å‹ä»…æ”¯æŒ Responses API                                 |
| **å¤æ‚åº¦**                | â­ ç®€å•ã€æ˜“ä¸Šæ‰‹             | â­â­ åŠŸèƒ½å¼ºã€ç»“æ„æ›´ä¸°å¯Œ                                             |
| **é€‚ç”¨åœºæ™¯**               | èŠå¤©æœºå™¨äººã€æ–‡æœ¬ç”Ÿæˆ           | æ™ºèƒ½åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ä»»åŠ¡ã€AI Agent                                      |
| **æœªæ¥å‘å±•æ–¹å‘**             | é•¿æœŸæŒç»­æ”¯æŒ               | OpenAI Agent ç”Ÿæ€çš„æ ¸å¿ƒ API                                   |
| **å®˜æ–¹æ€åº¦**               | ç»§ç»­ç»´æŠ¤                 | æ¨èä½œä¸ºä¸»åŠ›å¼€å‘æ¥å£      

                                         
**Text generation comparison in Python** 
 
![Alt Image Text](../images/chap8_0_1.png "Body image")


**Output Comparison**
                                         
![Alt Image Text](../images/chap8_0_2.png "Body image")  

## 2 Making Your First Call     

æœ¬èŠ‚å†…å®¹å›´ç»• Responses API çš„åŸºæœ¬ä½¿ç”¨æµç¨‹ä¸è¿”å›ç»“æœè§£æ å±•å¼€ï¼Œé€šè¿‡å®æ“æ¼”ç¤ºå¸®åŠ©å­¦ä¹ è€…ç†è§£ä»ç¯å¢ƒå‡†å¤‡åˆ°é¦–æ¬¡ API è°ƒç”¨çš„å®Œæ•´é“¾è·¯ã€‚

åœ¨å®é™…å·¥ç¨‹å®è·µä¸­ï¼Œæœ¬è¯¾ç¨‹ç¤ºä¾‹å·²è°ƒæ•´ä¸º ä½¿ç”¨ Ollama è¿è¡Œæœ¬åœ°æ¨¡å‹ï¼ˆå¦‚ qwen2.5:7bã€deepseek-r1ï¼‰ï¼Œä»¥æ›¿ä»£è¿œç¨‹ OpenAI APIã€‚è¿™ç§æ–¹å¼å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

æ¨ç†ä¸ç”Ÿæˆå®Œå…¨åœ¨æœ¬åœ°å®Œæˆï¼Œé€‚ç”¨äºå¯¹æ•°æ®éšç§å’Œç¦»çº¿èƒ½åŠ›è¦æ±‚è¾ƒé«˜çš„åœºæ™¯

API è°ƒç”¨æ–¹å¼åœ¨è¯­ä¹‰ä¸Šä¿æŒä¸€è‡´ï¼Œä½†ä¸ä¾èµ– OpenAI è´¦æˆ·ã€API Key æˆ–è®¡è´¹ä½“ç³»

éƒ¨åˆ† OpenAI Responses API çš„é«˜çº§ç‰¹æ€§ï¼ˆå¦‚ç»“æ„åŒ– reasoning å­—æ®µï¼‰åœ¨æœ¬åœ°æ¨¡å‹ä¸­ä»¥â€œæ–‡æœ¬æ¨ç†â€çš„å½¢å¼ä½“ç°ï¼Œéœ€è¦é€šè¿‡ Prompt è®¾è®¡è¿›è¡Œå¼•å¯¼

è¿™ä¸€å®è·µè·¯å¾„æ›´è´´è¿‘ä¼ä¸šå†…ç½‘éƒ¨ç½²ã€ç§æœ‰åŒ– AI Agent ç³»ç»Ÿå’Œæˆæœ¬å¯æ§çš„ç”Ÿäº§ç¯å¢ƒã€‚       


```
import json
from openai import OpenAI

client = OpenAI(
    api_key="ollama",  # éšä¾¿å†™
    base_url="http://localhost:11434/v1"
)
```

```
response = client.responses.create(
    model="qwen2.5:7b",
    input="Write one sentence about OpenAI Responses API."
)

print(response.output_text)
```

> The OpenAI Responses API enables developers to integrate sophisticated text generation and conversational AI capabilities into their applications by accessing a powerful language model.
           
### Instructions

```
response = client.responses.create(
    model="qwen2.5:7b",
    instructions="Talk like a pirate.",
    input="How are you?",
)

print(response.output_text)
```

> Arrr, matey! How be ye doin' today? Be ye feelin' swashbucklin' or perhaps ye be landlubberin'?

### Example with system + user:

```
response = client.responses.create(
    model="qwen2.5:7b",
    input=[
        {
            "role": "system",
            "content": "You are a helpful assistant who replies in bullet points.",
        },
        {"role": "user", "content": "Give me tips to focus better while studying."},
    ],
)
```

```
print(response.output_text)  # Main assistant reply (shortcut)
```

```
- Set clear goals for each study session
- Create a dedicated and organized study space
- Use a timer for focused work intervals (e.g., Pomodoro Technique)
- Minimize distractions by turning off notifications
- Maintain good posture to improve mental clarity
- Take regular breaks to avoid burnout
- Stay hydrated and eat healthily
- Use flashcards or quizzes to test retention
- Break large tasks into smaller, manageable parts
```

### Inspecting the response

```
print(response.output)  # Full structured output
```

> [ResponseOutputMessage(id='msg_619012', content=[ResponseOutputText(annotations=None, text='- Set clear goals for each study session\n- Create a dedicated and organized study space\n- Use a timer for focused work intervals (e.g., Pomodoro Technique)\n- Minimize distractions by turning off notifications\n- Maintain good posture to improve mental clarity\n- Take regular breaks to avoid burnout\n- Stay hydrated and eat healthily\n- Use flashcards or quizzes to test retention\n- Break large tasks into smaller, manageable parts', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')]


```
from pprint import pprint

pprint(response.model_dump())
```

```
{'background': None,
 'conversation': None,
 'created_at': 1765721564.0,
 'error': None,
 'id': 'resp_100363',
 'incomplete_details': None,
 'instructions': None,
 'max_output_tokens': None,
 'max_tool_calls': None,
 'metadata': None,
 'model': 'qwen2.5:7b',
 'object': 'response',
 'output': [{'content': [{'annotations': None,
                          'logprobs': None,
                          'text': '- Set clear goals for each study session\n'
                                  '- Create a dedicated and organized study '
                                  'space\n'
                                  '- Use a timer for focused work intervals '
                                  '(e.g., Pomodoro Technique)\n'
                                  '- Minimize distractions by turning off '
                                  'notifications\n'
                                  '- Maintain good posture to improve mental '
                                  'clarity\n'
                                  '- Take regular breaks to avoid burnout\n'
                                  '- Stay hydrated and eat healthily\n'
                                  '- Use flashcards or quizzes to test '
                                  'retention\n'
                                  '- Break large tasks into smaller, '
                                  'manageable parts',
                          'type': 'output_text'}],
             'id': 'msg_619012',
             'role': 'assistant',
             'status': 'completed',
             'type': 'message'}],
 'parallel_tool_calls': None,
 'previous_response_id': None,
 'prompt': None,
 'prompt_cache_key': None,
 'prompt_cache_retention': None,
 'reasoning': None,
 'safety_identifier': None,
 'service_tier': None,
 'status': 'completed',
 'temperature': None,
 'text': None,
 'tool_choice': None,
 'tools': None,
 'top_logprobs': None,
 'top_p': None,
 'truncation': None,
 'usage': {'input_tokens': 33,
           'input_tokens_details': None,
           'output_tokens': 90,
           'output_tokens_details': None,
           'total_tokens': 123},
 'user': None}
```
 
```
# You can also inspect:
print(response.usage.output_tokens)  # Token usage
print(response.usage.total_tokens)  # Total token usage
# print(response.reasoning.effort)
print(response.id)
```

```
90
123
resp_100363
```

### Using reasoning models

âœ… åªæœ‰ OpenAI å®˜æ–¹ reasoning æ¨¡å‹ æ‰æ”¯æŒï¼š

| OpenAI reasoning | æœ¬åœ°æ¨¡å‹ |
| ---------------- | ---- |
| æ¨ç† token å•ç‹¬è§£ç     | âŒ    |
| æ¨ç†ç»“æ„åŒ–è¿”å›          | âŒ    |
| æ§åˆ¶æ¨ç†å¼ºåº¦           | âŒ    |
| æ¨ç†ä¸å›ç­”åˆ†ç¦»          | âŒ    |

**æ—¢ç„¶æ¨¡å‹ä¸æ”¯æŒ reasoning.effortï¼Œé‚£å°±ç”¨è‡ªç„¶è¯­è¨€æç¤ºï¼š**

```
prompt = """
Please reason step by step and explain your logic clearly.

Question:
Whatâ€™s the next number in the sequence: 2, 6, 12, 20, ...?
"""


response = client.responses.create(
    model="deepseek-r1:1.5b",
    # reasoning={"effort": "medium"},
    input=[{"role": "user", "content": prompt}],
)

# âœ… åªæœ‰ OpenAI å®˜æ–¹ reasoning æ¨¡å‹ æ‰æ”¯æŒï¼š

# reasoning.effort

# response.reasoning

# å¯æ§æ¨ç†å¼ºåº¦

print(response.output_text)
```

```
<think>
First, I'll look at the given numbers: 2, 6, 12, 20.

Next, I'll calculate the differences between each consecutive pair to find a pattern:

- 6 - 2 = 4
- 12 - 6 = 6
- 20 - 12 = 8

The differences are 4, 6, and 8. These numbers increase by 2 each time.

To determine the next difference, I'll add 2 to the last difference:

4 (previous difference) + 2 = 6

Finally, I'll add this new difference to the last number in the sequence to find the fifth term:

20 + 6 = 26
</think>

**Solution:**

The given sequence is:  
2,â€ƒ6,â€ƒ12,â€ƒ20,â€ƒ...

To find the next number in the sequence, let's analyze the pattern.

1. **Calculate the Differences Between Consecutive Terms:**
   - First difference: \( 6 - 2 = 4 \)
   - Second difference: \( 12 - 6 = 6 \)
   - Third difference: \( 20 - 12 = 8 \)

2. **Identify the Pattern in Differences:**
   - The differences are increasing by **2** each time**:  
     4,â€ƒ6,â€ƒ8,â€ƒ...
   
3. **Predict the Next Difference:**
   - Following the pattern, the next difference will be \( 8 + 2 = 10 \).

4. **Determine the Next Term in the Sequence:**
   - Add this new difference to the last term:  
     \( 20 + 10 = 30 \)

**Final Answer:**  
\boxed{30}
```


```
print(response.usage)  # Token usage
# print(response.reasoning.effort)
```

> ResponseUsage(input_tokens=42, input_tokens_details=None, output_tokens=390, output_tokens_details=None, total_tokens=432)

Responses API ä¸ºæ„å»ºé¢å‘è¡ŒåŠ¨çš„ AI åº”ç”¨æä¾›äº†ç»Ÿä¸€æ¥å£ï¼Œè€Œåœ¨æœ¬åœ°åŒ–å®è·µä¸­ï¼Œç»“åˆ Ollama è¿è¡Œ qwen2.5 ä¸ deepseek-r1 ç­‰æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä¿æŒ Agent æ¶æ„ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå®ç°æ›´é«˜çš„æ•°æ®å¯æ§æ€§ä¸éƒ¨ç½²çµæ´»æ€§ã€‚

**é€šè¿‡è§£æ Responses API çš„è¿”å›ç»“æ„ï¼Œå¼€å‘è€…ä¸ä»…å¯ä»¥è·å–æ¨¡å‹è¾“å‡ºç»“æœï¼Œè¿˜èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°ç®¡ç†è¯·æ±‚çŠ¶æ€ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸æ¨ç†æˆæœ¬ï¼Œè¿™ä¸ºæ„å»ºå¤æ‚çš„ Agent å·¥ä½œæµå¥ å®šäº†å·¥ç¨‹åŸºç¡€**ã€‚

åœ¨æœ¬åœ°å¤§æ¨¡å‹ç¯å¢ƒä¸‹ï¼Œæ¨ç†èƒ½åŠ›æ›´å¤šä½“ç°åœ¨ç”Ÿæˆè¿‡ç¨‹æœ¬èº«ï¼Œè€Œéç»“æ„åŒ–å­—æ®µé…ç½®ï¼Œå› æ­¤ Prompt è®¾è®¡æˆä¸ºå½±å“æ¨ç†è´¨é‡ä¸å“åº”ç¨³å®šæ€§çš„å…³é”®å› ç´ ã€‚

ç›¸è¾ƒäºä¼ ç»Ÿçš„æ–‡æœ¬ç”Ÿæˆæ¥å£ï¼Œé¢å‘ Agent çš„ API è®¾è®¡æ›´å¼ºè°ƒâ€œè§„åˆ’â€”è¡ŒåŠ¨â€”åé¦ˆâ€çš„é—­ç¯ï¼Œè¿™ä¸€æ€æƒ³åŒæ ·é€‚ç”¨äºæœ¬åœ°å¤§æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚


## 3 ä¿ç•™å®¢æˆ·ç«¯å†å²

æœ¬æ®µå†…å®¹å›´ç»• **å¯¹è¯çŠ¶æ€ï¼ˆConversation Stateï¼‰ç®¡ç†** å±•å¼€ï¼Œç³»ç»Ÿé˜è¿°äº†å…¶é‡è¦æ€§ã€å®ç°æ–¹å¼ä»¥åŠåœ¨ Responses API ä¸­çš„å…·ä½“åº”ç”¨ä¸å–èˆã€‚

é¦–å…ˆå¼ºè°ƒäº†å¯¹è¯çŠ¶æ€çš„ä»·å€¼å¹¶ä¸ä»…é™äºâ€œè®°ä½ç”¨æˆ·è¯´è¿‡çš„è¯â€ï¼Œè€Œæ˜¯ç›´æ¥å½±å“åˆ°å¤šè½®äº¤äº’ä¸­çš„**å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ä¸æ¨ç†èƒ½åŠ›**ã€‚

è‰¯å¥½çš„ä¸Šä¸‹æ–‡ç®¡ç†å¯ä»¥å¸®åŠ©æ¨¡å‹åœ¨è¿ç»­å¯¹è¯ä¸­ä¿æŒæ­£ç¡®ç†è§£ï¼Œé™ä½é‡å¤è¾“å…¥å¸¦æ¥çš„ Token æ¶ˆè€—ä¸æˆæœ¬ï¼Œ**åŒæ—¶æ”¯æŒä»ä»»æ„å†å²èŠ‚ç‚¹è¿›è¡Œå¯¹è¯åˆ†æ”¯ï¼ˆforkï¼‰ï¼Œå¹¶ä¸ºæ›´å¤æ‚çš„æ¨ç†å’Œå·¥å…·è°ƒç”¨æ‰“ä¸‹åŸºç¡€**ã€‚

éšåï¼Œå†…å®¹é€šè¿‡ä»£ç ç¤ºä¾‹å±•ç¤ºäº†**æ‰‹åŠ¨ç®¡ç†ä¸Šä¸‹æ–‡**çš„åŸºæœ¬æ€è·¯ï¼šå°†ç”¨æˆ·ä¸åŠ©æ‰‹çš„æ¯ä¸€è½®æ¶ˆæ¯æŒ‰é¡ºåºå­˜å‚¨åœ¨å®¢æˆ·ç«¯ï¼Œå¹¶åœ¨æ¯æ¬¡è¯·æ±‚æ—¶å°†å®Œæ•´çš„æ¶ˆæ¯æ•°ç»„ä½œä¸ºè¾“å…¥å‘é€ç»™æ¨¡å‹ã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£å®Œæ•´çš„å¯¹è¯å†å²ï¼Œä»è€Œåœ¨â€œå†æ¥ä¸€ä¸ªâ€â€œç»§ç»­åˆšæ‰çš„è¯é¢˜â€ç­‰æ¨¡ç³ŠæŒ‡ä»¤ä¸‹ç»™å‡ºç¬¦åˆè¯­å¢ƒçš„å›åº”ã€‚è¿™ç§æ–¹å¼çµæ´»ã€å¯æ§ï¼Œä½†éœ€è¦å¼€å‘è€…è‡ªè¡Œç»´æŠ¤ä¸Šä¸‹æ–‡ç»“æ„ã€‚

åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä»‹ç»äº† Responses API æä¾›çš„ **è‡ªåŠ¨åŒ–ä¸Šä¸‹æ–‡ç®¡ç†æœºåˆ¶**ã€‚

é€šè¿‡åœ¨è¯·æ±‚ä¸­ä¼ å…¥ `previous_response_id`ï¼Œå¼€å‘è€…æ— éœ€æ‰‹åŠ¨æ‹¼æ¥å†å²æ¶ˆæ¯ï¼ŒAPI ä¼šè‡ªåŠ¨å°†å½“å‰è¯·æ±‚ä¸æŒ‡å®šçš„å†å²å“åº”ä¸²è”èµ·æ¥ï¼Œå½¢æˆåŒä¸€æ¡å¯¹è¯é“¾ã€‚**ç¤ºä¾‹ä¸­é€šè¿‡â€œè‹¹æœé—®é¢˜â€æ¸…æ™°åœ°æ¼”ç¤ºäº†å¤šè½®æ¨ç†èƒ½åŠ›ï¼šå³ä½¿åç»­é—®é¢˜ä¸­æœªæ˜¾å¼é‡å¤ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ¨¡å‹ä»èƒ½åŸºäºå…ˆå‰çŠ¶æ€ç»™å‡ºæ­£ç¡®ç»“æœ**ã€‚

Responses API çš„å¦ä¸€é¡¹é‡è¦èƒ½åŠ›æ˜¯ **å¯¹è¯åˆ†æ”¯ï¼ˆForkingï¼‰**ã€‚å¼€å‘è€…å¯ä»¥åŸºäºä»»æ„å†å²å“åº”åˆ›å»ºæ–°çš„å¯¹è¯åˆ†æ”¯ï¼Œè¿™ç§æœºåˆ¶ç±»ä¼¼äºåœ¨ ChatGPT Web UI ä¸­â€œç¼–è¾‘å†å²æ¶ˆæ¯åé‡æ–°ç”Ÿæˆâ€ï¼Œéå¸¸é€‚åˆæ¢ç´¢ä¸åŒæ¨ç†è·¯å¾„æˆ–å‡è®¾åœºæ™¯ã€‚

æœ€åï¼Œå†…å®¹æŒ‡å‡ºäº†è‡ªåŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„**æ½œåœ¨æˆæœ¬é—®é¢˜**ã€‚éšç€å¯¹è¯é“¾ä¸æ–­å¢é•¿ï¼Œè¾“å…¥ Token æ•°é‡ä¼šæŒç»­ç´¯ç§¯ï¼Œè¿›è€Œæ˜¾è‘—æé«˜è°ƒç”¨æˆæœ¬ï¼Œå¹¶æœ€ç»ˆè§¦åŠæ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸Šé™ã€‚å¯¹æ­¤ï¼Œè¯¾ç¨‹å±•ç¤ºäº†ä¸€ç§æŠ˜ä¸­æ–¹æ¡ˆï¼šé€šè¿‡åˆ é™¤æ—©æœŸå“åº”æ¥æˆªæ–­å¯¹è¯é“¾ï¼Œä»è€Œé™ä½ Token ä½¿ç”¨é‡ã€‚ä½†ä»£ä»·æ˜¯æ¨¡å‹å°†ä¸å†â€œè®°å¾—â€è¢«åˆ é™¤çš„å†å²å†…å®¹ã€‚ç”±æ­¤å¾—å‡ºç»“è®ºï¼šåœ¨å®é™…é¡¹ç›®ä¸­ï¼Œ**æ‰‹åŠ¨ç®¡ç†ä¸Šä¸‹æ–‡å¹¶ç»“åˆæˆªæ–­ã€æ‘˜è¦ç­‰ç­–ç•¥ï¼Œå¾€å¾€åœ¨æˆæœ¬ä¸æ§åˆ¶åŠ›ä¹‹é—´æ›´å…·ä¼˜åŠ¿**ã€‚



### å¯ç”¨äºæŠ€æœ¯æ–‡ç« çš„ä¸“ä¸šè¡¨è¿°ï¼ˆProfessional Linesï¼‰


> å¯¹è¯çŠ¶æ€ç®¡ç†ä¸ä»…å†³å®šäº†å¤šè½®äº¤äº’çš„è¿è´¯æ€§ï¼Œä¹Ÿç›´æ¥å½±å“æ¨¡å‹æ¨ç†å‡†ç¡®æ€§ã€Token ä½¿ç”¨æ•ˆç‡ä»¥åŠæ•´ä½“è°ƒç”¨æˆæœ¬ã€‚

> Responses API é€šè¿‡ `previous_response_id` æä¾›äº†è‡ªåŠ¨åŒ–çš„ä¸Šä¸‹æ–‡ä¸²è”æœºåˆ¶ï¼Œä½¿å¤šè½®å¯¹è¯ä¸åˆ†æ”¯æ¢ç´¢åœ¨å·¥ç¨‹å®ç°ä¸Šæ›´åŠ ç®€æ´ã€‚

> å¯¹è¯åˆ†æ”¯èƒ½åŠ›ä¸ºå¼€å‘è€…æä¾›äº†ç±»ä¼¼ç‰ˆæœ¬æ§åˆ¶çš„äº¤äº’ä½“éªŒï¼Œèƒ½å¤Ÿåœ¨åŒä¸€ä¸Šä¸‹æ–‡åŸºç¡€ä¸Šå¹¶è¡Œæ¢ç´¢ä¸åŒæ¨ç†è·¯å¾„ã€‚

> è‡ªåŠ¨ä¸Šä¸‹æ–‡ç®¡ç†è™½ç„¶é™ä½äº†å¼€å‘å¤æ‚åº¦ï¼Œä½†éšç€å¯¹è¯é“¾å¢é•¿ï¼ŒToken æˆæœ¬ä¹Ÿä¼šçº¿æ€§ä¸Šå‡ï¼Œå› æ­¤åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦ç»“åˆæˆªæ–­æˆ–æ‘˜è¦ç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚

> åœ¨å¤æ‚æˆ–é•¿å‘¨æœŸå¯¹è¯åœºæ™¯ä¸‹ï¼Œæ˜¾å¼çš„å®¢æˆ·ç«¯ä¾§ä¸Šä¸‹æ–‡ç®¡ç†å¾€å¾€èƒ½åœ¨å¯æ§æ€§ä¸æˆæœ¬ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚


### å¦‚ä½•ç”¨æœ¬åœ°æ¨¡å‹å®ç°ä¿ç•™å®¢æˆ·ç«¯å†å²

#### â— æœ¬åœ° Ollama / LLM çš„åŸºæœ¬äº‹å®

| èƒ½åŠ›                 | æ˜¯å¦å­˜åœ¨ |
| ------------------ | ---- |
| è‡ªåŠ¨è®°ä½ä¸Šä¸€æ¬¡è¯·æ±‚          | âŒ    |
| å®¢æˆ·ç«¯è‡ªåŠ¨ä¼šè¯ç®¡ç†          | âŒ    |
| ç±»ä¼¼ ChatGPT çš„â€œå¯¹è¯è®°å¿†â€ | âŒ    |
| æœåŠ¡ç«¯ä¿å­˜ conversation | âŒ    |

ğŸ‘‰ **æ¯ä¸€æ¬¡è°ƒç”¨éƒ½æ˜¯å…¨æ–°çš„æ¨ç†**

---

### äºŒã€ä¸ºä»€ä¹ˆä½ â€œä»¥ä¸ºâ€å®ƒä¼šæœ‰å†å²ï¼Ÿ

ä½ å¾ˆå¯èƒ½é‡åˆ°äº†ä¸‹é¢ä¹‹ä¸€ï¼ˆæˆ–å¤šä¸ªï¼‰ğŸ‘‡

#### âŒ è¯¯åŒº 1ï¼šä»¥ä¸º `client` ä¼šä¿å­˜ä¸Šä¸‹æ–‡

```python
client = OpenAI(...)
```

æˆ–ï¼ˆOllama å…¼å®¹å†™æ³•ï¼‰

```python
client = OpenAI(base_url="http://localhost:11434/v1")
```

ğŸ‘‰ **client åªæ˜¯ HTTP å®¢æˆ·ç«¯ï¼Œä¸æ˜¯ä¼šè¯å¯¹è±¡**

å®ƒä¸ä¼šä¿å­˜ï¼š

* ä¸Šä¸€æ¬¡ prompt
* ä¸Šä¸€æ¬¡ response
* ä¸Šä¸€æ¬¡ token

### âŒ è¯¯åŒº 2ï¼šä»¥ä¸º Responses API / Chat API ä¼šâ€œè‡ªåŠ¨æ¥ç€èŠâ€

æ¯”å¦‚ç±»ä¼¼ï¼š

```python
response = client.responses.create(
    model="qwen2.5:7b",
    input="ç»§ç»­åˆšæ‰çš„è¯"
)
```

æ¨¡å‹å®é™…æ”¶åˆ°çš„æ˜¯ï¼š

```
â€œç»§ç»­åˆšæ‰çš„è¯â€
```

ä½†å®ƒ**æ ¹æœ¬ä¸çŸ¥é“â€œåˆšæ‰çš„è¯â€æ˜¯ä»€ä¹ˆ**ã€‚


#### âŒ è¯¯åŒº 3ï¼šæŠŠ OpenAI Assistants / ChatGPT çš„ä½“éªŒï¼Œè¯¯è®¤ä¸ºæ˜¯æ¨¡å‹èƒ½åŠ›

ChatGPT èƒ½â€œè®°ä½â€æ˜¯å› ä¸ºï¼š

* æµè§ˆå™¨åœ¨**ä¸æ–­æŠŠå®Œæ•´å†å²å‘å›æœåŠ¡å™¨**
* ä½ çœ‹åˆ°çš„æ˜¯ **UI å±‚åšçš„çŠ¶æ€ç®¡ç†**
* ä¸æ˜¯æ¨¡å‹è‡ªå·±â€œè®°å¾—â€

---

### ä¸‰ã€99% å‡ºé—®é¢˜çš„ä»£ç ç»“æ„ï¼ˆä½ å¯ä»¥å¯¹ç…§ï¼‰

#### âŒ å¸¸è§é”™è¯¯å†™æ³•ï¼ˆæ²¡æœ‰å†å²ï¼‰

```python
def ask_llm(prompt):
    response = client.responses.create(
        model="deepseek-r1",
        input=prompt
    )
    return response.output_text
```

ç„¶åï¼š

```python
ask_llm("ä½ å¥½")
ask_llm("æˆ‘åˆšæ‰è¯´äº†ä»€ä¹ˆï¼Ÿ")
```

ğŸ‘‰ ç¬¬äºŒæ¬¡è°ƒç”¨ **å®Œå…¨ä¸çŸ¥é“ç¬¬ä¸€æ¬¡å‘ç”Ÿäº†ä»€ä¹ˆ**

### å››ã€æ­£ç¡®åšæ³•ï¼ˆå¿…é¡»ä½ è‡ªå·±ç»´æŠ¤å†å²ï¼‰

#### âœ… æ­£ç¡®æ¨¡å¼ï¼šå®¢æˆ·ç«¯æ˜¾å¼ä¿å­˜ history

#### 1ï¸âƒ£ ç”¨ messages / input åˆ—è¡¨ç´¯ç§¯ä¸Šä¸‹æ–‡

```python
conversation = []

def chat(user_input):
    conversation.append({
        "role": "user",
        "content": user_input
    })

    response = client.responses.create(
        model="qwen2.5:7b",
        input=conversation
    )

    assistant_reply = response.output_text

    conversation.append({
        "role": "assistant",
        "content": assistant_reply
    })

    return assistant_reply
```

è¿™æ ·æ‰æ˜¯**çœŸæ­£çš„â€œå¯¹è¯â€**ã€‚

---

#### 2ï¸âƒ£ å¦‚æœä½ ç”¨çš„æ˜¯ Chat Completions é£æ ¼

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."}
]

def chat(user_msg):
    messages.append({"role": "user", "content": user_msg})

    resp = client.chat.completions.create(
        model="qwen2.5:7b",
        messages=messages
    )

    reply = resp.choices[0].message.content
    messages.append({"role": "assistant", "content": reply})
    return reply
```


### äº”ã€ä¸ºä»€ä¹ˆ Responses API é‡Œçš„ `response.id` ä¹Ÿæ²¡å¸®ä½ ï¼Ÿ

ä½ å¯èƒ½æ³¨æ„åˆ°è¿”å›é‡Œæœ‰ï¼š

```json
"id": "resp-xxxxx"
```

âš ï¸ **æ³¨æ„è¿™ä¸ªé‡è¦äº‹å®**ï¼š

> åœ¨ Ollama / æœ¬åœ°æ¨¡å‹ä¸­ï¼Œ`response.id`
> âŒ ä¸ä»£è¡¨ä¼šè¯
> âŒ ä¸ä»£è¡¨ conversation
> âŒ ä¸èƒ½è‡ªåŠ¨ç»­èŠ

å®ƒåªæ˜¯ä¸€æ¬¡è¯·æ±‚çš„ IDï¼Œç”¨äºæ—¥å¿—æˆ–è°ƒè¯•ã€‚

---

### å…­ã€å¦‚æœä½ æƒ³è¦â€œæ›´åƒ ChatGPTâ€çš„ä½“éªŒï¼Œè¯¥æ€ä¹ˆåšï¼Ÿ

#### æ¨è 3 ä¸ªå·¥ç¨‹çº§æ–¹æ¡ˆï¼ˆä»ç®€å•åˆ°ä¸“ä¸šï¼‰

---

#### âœ… æ–¹æ¡ˆ 1ï¼šæœ€ç®€å•ï¼ˆåˆ—è¡¨ç´¯ç§¯ï¼‰

é€‚åˆ Notebook / Demo / å­¦ä¹ 

```python
history = []
```

#### âœ… æ–¹æ¡ˆ 2ï¼šConversation Managerï¼ˆæ¨èï¼‰

```python
class Conversation:
    def __init__(self, system_prompt):
        self.messages = [{"role": "system", "content": system_prompt}]

    def ask(self, user_input):
        self.messages.append({"role": "user", "content": user_input})
        resp = client.responses.create(
            model="deepseek-r1",
            input=self.messages
        )
        reply = resp.output_text
        self.messages.append({"role": "assistant", "content": reply})
        return reply
```


#### ğŸš€ æ–¹æ¡ˆ 3ï¼šAgent æ¶æ„ï¼ˆä½ å·²ç»å¾ˆæ¥è¿‘äº†ï¼‰

* Memory æ¨¡å—
* Planner
* Tool Router
* History Truncationï¼ˆé˜² token çˆ†ç‚¸ï¼‰

### ä¸ƒã€ä¸ºä»€ä¹ˆè¿™ä¸ªé—®é¢˜**éå¸¸å…³é”®**ï¼ˆè¯´å¥å®è¯ï¼‰

ä½ ç°åœ¨é—®çš„è¿™ä¸ªé—®é¢˜ï¼Œ**æ­£å¥½æ˜¯ä»â€œä¼šè°ƒç”¨æ¨¡å‹â€ â†’ â€œä¼šåš Agent ç³»ç»Ÿâ€ çš„åˆ†æ°´å²­**ã€‚

> **90% çš„åˆå­¦è€…éƒ½ä»¥ä¸ºæ˜¯æ¨¡å‹ä¸è¡Œ
> å…¶å®æ˜¯è‡ªå·±æ²¡åšçŠ¶æ€ç®¡ç†**

###  Conversation State with OpenAI API

**Why Conversation State Is Critical**

* Accuracy across multiple turns
* Reducing token usage
* Fork and continue
* Deeper reasoning and tool use

```
import json
from openai import OpenAI

with open("../config.json") as f:
    config = json.load(f)

# Access API key
api_key = config["openai_api_key"]

client = OpenAI(api_key=api_key)
```

#### Simulate a conversation

```
response = client.responses.create(
    model="gpt-4.1",
    input=[
        {"role": "user", "content": "What is 5+5?"},
        {"role": "assistant", "content": "10"},
        {"role": "user", "content": "Now subtract 2 from it."},
    ],
)

print(response.output_text)
```

> 10 - 2 = **8**

#### Keep a client side history

```
history = [{"role": "user", "content": "tell me a joke"}]

response = client.responses.create(model="gpt-4o-mini", input=history, store=False)

print(response.output_text)

# Add the response to the conversation
history += [{"role": el.role, "content": el.content} for el in response.output]

history.append({"role": "user", "content": "tell me another"})

second_response = client.responses.create(
    model="gpt-4o-mini", input=history, store=False
)

print(second_response.output_text)
```

Output

```
Why did the scarecrow win an award?  

Because he was outstanding in his field!
Why donâ€™t skeletons fight each other?  

They donâ€™t have the guts!
```

#### OpenAI APIs for conversation state

```
# Message 1: John receives apples
first_response = client.responses.create(
    model="gpt-4o",
    input="John has 5 apples. Emma has 10. Emma gives him 3 apples. How many apples do John and Emma have now?",
)
print("Response 1:", first_response.output_text)
```

```
10 - 2 = **8**

Why did the scarecrow win an award?  

Because he was outstanding in his field!
Why donâ€™t skeletons fight each other?  

They donâ€™t have the guts!
Response 1: Initially, John has 5 apples and Emma has 10 apples. If Emma gives John 3 apples:

- John will have: 5 + 3 = 8 apples
- Emma will have: 10 - 3 = 7 apples

So, John now has 8 apples and Emma has 7 apples.
```

```
# Message 2: John eats apples, continuing the thread
second_response = client.responses.create(
    model="gpt-4o-mini",
    previous_response_id=first_response.id,
    input=[
        {
            "role": "user",
            "content": "Then he eats 2 apples. How many does he have left?",
        }
    ],
)
print("Response 2:", second_response.output_text)
```

```
Response 2: If John has 8 apples and he eats 2, he will have:

8 - 2 = 6 apples left.

So, John now has 6 apples.
```

```
# Message 3: John gives half to Emma
third_response = client.responses.create(
    model="gpt-4o-mini",
    previous_response_id=second_response.id,
    input=[
        {
            "role": "user",
            "content": "He gives half of them to Emma. How many apples does each person have now?",
        }
    ],
)
print("Response 3:", third_response.output_text)
```

```
10 - 2 = **8**

Why did the scarecrow win an award?  

Because he was outstanding in his field!
Why donâ€™t skeletons fight each other?  

They donâ€™t have the guts!
Response 1: Initially, John has 5 apples and Emma has 10 apples. If Emma gives John 3 apples:

- John will have: 5 + 3 = 8 apples
- Emma will have: 10 - 3 = 7 apples

So, John now has 8 apples and Emma has 7 apples.
Response 2: If John has 8 apples and he eats 2, he will have:

8 - 2 = 6 apples left.

So, John now has 6 apples.
Response 3: If John has 6 apples and he gives half to Emma, he will give away:

6 / 2 = 3 apples.

Now, after giving 3 apples to Emma:

- John's apples: 6 - 3 = 3 apples
- Emma's apples: 7 + 3 = 10 apples

So, John has 3 apples and Emma has 10 apples now.
```

#### Fork a response at any point

```
# Message 2: John eats apples, continuing the thread
forked_second_response = client.responses.create(
    model="gpt-4o-mini",
    previous_response_id=first_response.id,
    input=[
        {
            "role": "user",
            "content": "Then he eats 1 apple. How many does he have left?",
        }
    ],
)
print("Response 2:", forked_second_response.output_text)
```

```
Response 2: If John eats 1 apple, he will have:

8 - 1 = 7 apples left.

So, John has 7 apples now.
```

> Info: Even when using previous_response_id, all previous input tokens for responses in the chain are billed as input tokens in the API.

```
first_response.usage
```

> ResponseUsage(input_tokens=35, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=68, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=103)

```
second_response.usage
```

> ResponseUsage(input_tokens=124, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=36, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=160)


```
third_response.usage
```

> ResponseUsage(input_tokens=184, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=88, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=272)

```
client.responses.delete(first_response.id)
```

```
# Message 3: John gives half to Emma

third_response = client.responses.create(
    model="gpt-4o-mini",
    previous_response_id=second_response.id,
    input=[
        {
            "role": "user",
            "content": "He gives half of them to Emma. How many apples does each person have now?",
        }
    ],
)
print("Response 3:", third_response.output_text)
```

```
Response 3: If John has 6 apples and he gives half to Emma, he gives away:

6 Ã· 2 = 3 apples.

Now, John has:

6 - 3 = 3 apples.

So, after the transaction:

- John has 3 apples.
- Emma has 3 apples.

Each person has 3 apples.
```

```
third_response.usage
```

> ResponseUsage(input_tokens=81, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=69, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=150)


### äºŒã€ä¸ºä»€ä¹ˆè¿™æ®µä»£ç ã€Œåœ¨ OpenAI å®˜æ–¹ API èƒ½å·¥ä½œï¼Œä½†åœ¨ Ollama ä¸€å®šä¸è¡Œã€

âœ… åœ¨ OpenAI Responses API ä¸­

* previous_response_id
* å†…éƒ¨ state
* tool calls
* MCP

ğŸ‘‰ è¿™äº›æ˜¯â€œæœåŠ¡ç«¯èƒ½åŠ›â€

âŒ åœ¨ Ollama + æœ¬åœ°æ¨¡å‹ä¸­

* æ²¡æœ‰ session
* æ²¡æœ‰ server-side memory
* æ²¡æœ‰ response graph
* æ²¡æœ‰ response_id é“¾æ¥æœºåˆ¶

ğŸ‘‰ æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼Œå¿…é¡»ä½ è‡ªå·±ä¼ 

ä¸€å¥è¯æ€»ç»“ï¼š

**Responses API çš„â€œå¯¹è¯è¿ç»­æ€§â€ä¸æ˜¯åè®®ç‰¹æ€§ï¼Œè€Œæ˜¯ OpenAI æœåŠ¡å®ç°**

```
history = []

def ask(user_input):
    history.append({"role": "user", "content": user_input})

    response = client.responses.create(
        model="qwen2.5:7b",
        input=history,
    )

    history.append({
        "role": "assistant",
        "content": response.output_text
    })

    return response.output_text
```


```
ask("John has 5 apples. Emma has 10. Emma gives him 3 apples.")
ask("Then John eats 2 apples. How many does he have left?")
ask("John gives half of them to Emma. How many apples does each person have now?")
```
ğŸ‘‰ ç°åœ¨æ¨¡å‹çœŸçš„â€œè®°ä½â€äº†

```
"Sure, let's go through the steps:\n\n1. 

After receiving 3 apples from Emma and eating 2, John has 8 - 2 = 6 apples.\n2. 

John then gives half of his 6 apples to Emma.\n\n

Half of 6 apples is:\n\\[ \\frac{6}{2} = 3 \\text{ apples} \\]\n\n

So, John gives 3 apples to Emma. Now let's update the number of apples each person has:\n\n- John initially had 6 apples and gave away 3, so he now has:\n  \\[ 6 - 3 = 3 \\text{ apples} \\]\n- Emma originally had 10 apples, received 3 from John, and did not eat any, 

so she now has:\n  \\[ 10 + 3 = 13 \\text{ apples} \\]\n\nTherefore, after the exchange, John has 3 apples and Emma has 13 apples."
```

##  Integrating Tools: Web, Functions, Code

**Available Tools**

* Function calling
* Web search
* File search
* Image generation
* Code interpreter
* Computer use


### ä»æ–‡æœ¬åˆ°è¡ŒåŠ¨ï¼šResponses API å·¥å…·èƒ½åŠ›å…¨è§£æ

éšç€å¤§æ¨¡å‹é€æ¸ä»â€œå¯¹è¯åŠ©æ‰‹â€æ¼”è¿›ä¸ºâ€œæ‰§è¡Œå‹æ™ºèƒ½ä½“ï¼ˆAgentï¼‰â€ï¼Œä»…å…·å¤‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å·²ç»è¿œè¿œä¸å¤Ÿã€‚OpenAI æ¨å‡ºçš„ **Responses API** æ­£æ˜¯å›´ç»•è¿™ä¸€è¶‹åŠ¿ï¼Œå°†å¤šç§å…³é”®å·¥å…·èƒ½åŠ›æ•´åˆè¿›ç»Ÿä¸€æ¥å£ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨åˆé€‚çš„æ—¶æœºä¸»åŠ¨è°ƒç”¨å¤–éƒ¨èƒ½åŠ›ï¼Œå®ŒæˆçœŸå®ä¸–ç•Œä¸­çš„å¤æ‚ä»»åŠ¡ã€‚

###  å·¥å…·å³èƒ½åŠ›ï¼šResponses API çš„æ ¸å¿ƒè®¾è®¡ç†å¿µ

åœ¨ Responses API ä¸­ï¼Œâ€œå·¥å…·ï¼ˆToolsï¼‰â€å¹¶éé™„åŠ åŠŸèƒ½ï¼Œè€Œæ˜¯æ¨¡å‹æ¨ç†æµç¨‹ä¸­çš„ä¸€ç­‰å…¬æ°‘ã€‚å®ƒä»¬åŒ…æ‹¬å‡½æ•°è°ƒç”¨ã€ç½‘é¡µæœç´¢ã€æ–‡ä»¶ä¸å›¾åƒå¤„ç†ã€ä»£ç è§£é‡Šå™¨ä»¥åŠè®¡ç®—æœºæ“ä½œèƒ½åŠ›ï¼Œå…±åŒæ„æˆäº†ç°ä»£ AI Agent çš„æ‰§è¡Œå±‚ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹ä¸å†åªæ˜¯ç”Ÿæˆç­”æ¡ˆï¼Œè€Œæ˜¯èƒ½å¤Ÿ**æ„ŸçŸ¥éœ€æ±‚ã€è§„åˆ’æ­¥éª¤å¹¶æ‰§è¡ŒåŠ¨ä½œ**ã€‚

### å‡½æ•°è°ƒç”¨ï¼šè¿æ¥æ¨¡å‹ä¸çœŸå®ç³»ç»Ÿçš„æ¡¥æ¢

å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰æ˜¯æ„å»ºè‡ªåŠ¨åŒ–ç³»ç»Ÿå’Œ Agent å·¥ä½œæµçš„åŸºç¡€èƒ½åŠ›ã€‚å¼€å‘è€…å¯ä»¥é€šè¿‡æ¸…æ™°ã€ä¸¥æ ¼çš„å‡½æ•°æè¿°å’Œå‚æ•°å®šä¹‰ï¼Œè®©æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€è¾“å…¥ä¸‹è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å‡½æ•°ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„è°ƒç”¨å‚æ•°ã€‚è¿™ä¸€æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿå®‰å…¨ã€å¯æ§åœ°è®¿é—®å¤–éƒ¨ API æˆ–ä¸šåŠ¡ç³»ç»Ÿï¼Œè€Œä¸ä¾èµ–è„†å¼±çš„æ–‡æœ¬è§£æé€»è¾‘ã€‚

### ç½‘é¡µæœç´¢ï¼šè·å–å®æ—¶ä¿¡æ¯ï¼Œä½†éœ€è¦æˆæœ¬æ„è¯†

Responses API å†…ç½®çš„ç½‘é¡µæœç´¢å·¥å…·ä¸ºæ¨¡å‹æä¾›äº†å®æ—¶ä¿¡æ¯è·å–èƒ½åŠ›ï¼Œé€‚ç”¨äºæ–°é—»æ‘˜è¦ã€æœ¬åœ°æ¨èç­‰åœºæ™¯ã€‚ä½†å®è·µä¸­éœ€è¦æ³¨æ„ï¼Œæœç´¢æ·±åº¦å’Œä¸Šä¸‹æ–‡è§„æ¨¡ä¼šæ˜¾è‘—å½±å“å“åº”æ—¶é—´ä¸æˆæœ¬ã€‚é€šè¿‡åˆç†æ§åˆ¶ context sizeï¼Œå¯ä»¥åœ¨ä¿¡æ¯è´¨é‡ã€å»¶è¿Ÿå’Œè´¹ç”¨ä¹‹é—´å–å¾—æ›´ä¼˜å¹³è¡¡ï¼Œè¿™ä¹Ÿæ˜¯å·¥ç¨‹åŒ–è½åœ°æ—¶ä¸å¯å¿½è§†çš„ä¸€ç¯ã€‚

### å›¾åƒç”Ÿæˆä¸å¤šè½®ç¼–è¾‘ï¼šä»ä¸€æ¬¡ç”Ÿæˆåˆ°æŒç»­åˆ›ä½œ

åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒResponses API ä¸ä»…æ”¯æŒé«˜è´¨é‡çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼Œè¿˜æ”¯æŒåŸºäºå†å²å“åº”æˆ–å›¾åƒ ID çš„å¤šè½®ç¼–è¾‘ã€‚è¿™ç§â€œå¯è¿½æº¯ã€å¯è¿­ä»£â€çš„è®¾è®¡ï¼Œä½¿å›¾åƒç”Ÿæˆæ›´æ¥è¿‘çœŸå®åˆ›ä½œæµç¨‹ï¼Œé€‚ç”¨äºè®¾è®¡è¿­ä»£ã€åˆ›æ„ç”Ÿæˆå’Œäº§å“åŸå‹ç­‰åœºæ™¯ã€‚

### Code Interpreterï¼šè®©æ¨¡å‹â€œä¼šç®—ã€èƒ½è·‘ã€å¯è§£é‡Šâ€

ä»£ç è§£é‡Šå™¨ï¼ˆPython Toolï¼‰ä¸ºæ¨¡å‹æä¾›äº†â€œå†™ä»£ç  + æ‰§è¡Œä»£ç  + è§£é‡Šç»“æœâ€çš„é—­ç¯èƒ½åŠ›ã€‚ç›¸è¾ƒäºç›´æ¥ç»™å‡ºç»“è®ºï¼Œè¿™ç§æ–¹å¼æ˜¾è‘—æå‡äº†ç»“æœçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå°¤å…¶é€‚åˆæ•°å­¦æ¨å¯¼ã€æ•°æ®åˆ†æå’Œå·¥ç¨‹è®¡ç®—ç­‰å¯¹å‡†ç¡®æ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ã€‚

###  Code  - tools.ipynb

```
import json
from openai import OpenAI

with open("../config.json") as f:
    config = json.load(f)

# Access API key
api_key = config["openai_api_key"]

client = OpenAI(
     base_url="https://openrouter.ai/api/v1",
     api_key=api_key
    )
```

#### Function Calling

```
import requests


def get_weather(latitude, longitude):
    response = requests.get(
        f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m"
    )
    data = response.json()
    return data["current"]["temperature_2m"]
```
 
``` 
 tools = [
    {
        "type": "function",
        "name": "get_weather",
        "description": "Get current temperature for provided coordinates in celsius.",
        "parameters": {
            "type": "object",
            "properties": {
                "latitude": {"type": "number"},
                "longitude": {"type": "number"},
            },
            "required": ["latitude", "longitude"],
            "additionalProperties": False,
        },
        "strict": True,
    }
]

input_messages = [
    {"role": "user", "content": "What's the weather like in Paris today?"}
]

response = client.responses.create(
    model="arcee-ai/trinity-mini:free",
    input=input_messages,
    tools=tools,
```    
 
```
for r in response.output:
    print("Function call:", r.name)
    print("Arguments:", r.arguments)
    print("Status:", r.status)
```

```
Function call: get_weather
Arguments: {"latitude":48.8566,"longitude":2.3522}
Status: completed
``` 

```
import json

if r.name == "get_weather":
    args = json.loads(r.arguments)  # parse string â†’ dict
    latitude = args["latitude"]
    longitude = args["longitude"]
    temperature = get_weather(latitude, longitude)
    print(f"The current temperature in Paris is {temperature}Â°C.")
```

> The current temperature in Paris is 11.3Â°C.

#### Web search

```
response = client.responses.create(
    model="openai/gpt-oss-20b:free",
    tools=[{"type": "web_search_preview"}],
    input="What is the biggest tech news today in one sentence?",
)

print(response.output_text)
```

> IBM has agreed to buy dataâ€‘streaming platform Confluent for $11â€¯billion in cash, a move that will bolster its AI and dataâ€‘automation portfolio and make it the largest acquisition in the companyâ€™s recent historyâ€¯[techcrunch.com](https://techcrunch.com/2025/12/08/ibm-to-acquire-confluent-for-11b-as-it-seeks-to-bolster-its-data-offerings/)[prnewswire.com](https://www.prnewswire.com/news-releases/ibm-to-acquire-confluent-to-create-smart-data-platform-for-enterprise-generative-ai-302635317.html).

```
print(response.output)
```


```
 [ResponseReasoningItem(id='rs_tmp_sx0e1tf4cd', summary=[], type='reasoning', content=[Content(text='We need to answer: "What is the biggest tech news today in one sentence?" We have to incorporate web search results. The search results include Reuters article about global tech shares surge due to Nvidia\'s AI boom, IBM acquiring Confluent for $11B, Google antitrust remedies, etc. The question: "What is the biggest tech news today in one sentence?" We need to pick the biggest news. Likely the IBM acquisition of Confluent is big. Or the Google antitrust remedies. Which is bigger? IBM acquiring Confluent for $11B is a major deal. Google antitrust remedies also significant. But the question: "today" refers to the date of the search: 2025-12-16. The IBM acquisition was announced Dec 8, 2025. Google antitrust remedies finalized Dec 5, 2025. Which is bigger? IBM acquisition is a big $11B deal. Google antitrust is also huge. But the question: "biggest tech news today" likely refers to the most significant headline. The Reuters article about global tech shares surge due to Nvidia\'s AI boom is also big. But the IBM acquisition is a major corporate event. Let\'s choose IBM acquiring Confluent for $11B. Provide one sentence summarizing. Cite sources: TechCrunch article and PRNewswire. Use markdown links named by domain: techcrunch.com, prnewswire.com. Also maybe mention Reuters? But we can cite TechCrunch and PRNewswire. Provide one sentence: "IBM has agreed to acquire dataâ€‘streaming platform Confluent for $11\u202fbillion in cash, positioning the tech giant to strengthen its AI and data offerings." Cite.', type='reasoning_text')], encrypted_content=None, status=None), ResponseOutputMessage(id='msg_tmp_xexiao43uc', content=[ResponseOutputText(annotations=[AnnotationURLCitation(end_index=0, start_index=0, title="Today's Latest Technology News | Reuters", type='url_citation', url='https://www.reuters.com/technology/'), AnnotationURLCitation(end_index=0, start_index=0, title='IBM to acquire Confluent for $11B as it seeks to bolster its data offerings', type='url_citation', url='https://techcrunch.com/2025/12/08/ibm-to-acquire-confluent-for-11b-as-it-seeks-to-bolster-its-data-offerings/'), AnnotationURLCitation(end_index=0, start_index=0, title='IBM to Acquire Confluent to Create Smart Data Platform for Enterprise Generative AI', type='url_citation', url='https://www.prnewswire.com/news-releases/ibm-to-acquire-confluent-to-create-smart-data-platform-for-enterprise-generative-ai-302635317.html'), AnnotationURLCitation(end_index=0, start_index=0, title='Judge finalizes remedies in Google antitrust case', type='url_citation', url='https://www.cnbc.com/2025/12/05/judge-finalize-remedies-in-google-antitrust-case.html'), AnnotationURLCitation(end_index=0, start_index=0, title='Tech - CNBC', type='url_citation', url='https://www.cnbc.com/technology/')], text='IBM has agreed to buy dataâ€‘streaming platform Confluent for $11\u202fbillion in cash, a move that will bolster its AI and dataâ€‘automation portfolio and make it the largest acquisition in the companyâ€™s recent history\u202f[techcrunch.com](https://techcrunch.com/2025/12/08/ibm-to-acquire-confluent-for-11b-as-it-seeks-to-bolster-its-data-offerings/)[prnewswire.com](https://www.prnewswire.com/news-releases/ibm-to-acquire-confluent-to-create-smart-data-platform-for-enterprise-generative-ai-302635317.html).', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')]
```

```
## User location based search

response = client.responses.create(
    model="openai/gpt-oss-120b:free",
    tools=[
        {
            "type": "web_search_preview",
            "user_location": {
                "type": "approximate",
                "country": "GB",
                "city": "London",
                "region": "London",
            },
        }
    ],
    input="What are the best restaurants around Wimbledon?",
)

print(response.output_text)
```

```
Below is a quickâ€‘reference guide to the restaurants that consistently rank highest (by diners, critics and booking data) in and around Wimbledon.  Iâ€™ve grouped them by the type of experience you might be after â€“â€¯fineâ€‘dining, casualâ€‘bistro, familyâ€‘friendly, ethnic specialties, and â€œmustâ€‘tryâ€ hidden gems â€“â€¯and added the key details that most visitors care about (cuisine, price band, OpenTable rating or Infatuation score, and a oneâ€‘sentence why itâ€™s worth a reservation).

---

## 1.  Fineâ€‘Dining / â€œSpecialâ€‘Occasionâ€ Picks  

| Restaurant | Cuisine | Price* | Rating (OpenTable / Infatuation) | Why it stands out |
|------------|---------|--------|----------------------------------|-------------------|
| **Buenosâ€¯Aires Argentine Steakhouse** â€“ Wimbledon | Argentinian grill | Â£Â£Â£Â£ (expensive) | 4.8â€¯/â€¯4.8â€¯â­ (OT) | Championâ€‘grade charâ€‘grilled steaks; highlighted as OpenTableâ€™s â€œDinersâ€™ top pickâ€ for its quality and governmentâ€‘backed Argentine flagship statusã€opentable.comã€‘ | 
| **Efesâ€¯Premium** â€“ Wimbledon | Turkish / Mediterranean | Â£Â£Â£Â£ (expensive) | 4.7â€¯/â€¯4.7â€¯â­ (OT) | Upscale Turkish tasting menu with a modern dÃ©cor; praised for fresh ingredients and a romantic vibeã€opentable.comã€‘ | 
| **The Fox and Grapes** â€“ Wimbledon | Modern British gastroâ€‘pub | Â£Â£Â£ (moderate) | 4.5â€¯/â€¯4.5â€¯â­ (OT) | Bibâ€¯Gourmandâ€‘awarded, seasonal British dishes and a Josperâ€‘oven steak; a favourite of tennis fans and locals alikeã€opentable.comã€‘ | 
| **The Rushmere** (formerly The Swan) â€“ Wimbledon | Gastroâ€‘pub, liveâ€‘sport TV | Â£Â£Â£Â£ (expensive) | 4.4â€¯/â€¯4.5â€¯â­ (OT) | Large screens for sport, hearty British fare and a lively bar atmosphereã€opentable.comã€‘ | 
| **Takahashi** â€“ South Wimbledon | Omakaseâ€‘style Japanese (12â€‘seat) | Â£Â£Â£Â£ (expensive) | 4.8â€¯/â€¯4.7â€¯â­ (Infatuation) | Intimate, chefâ€‘directed tasting menu that feels private without the ultraâ€‘price tag of centralâ€‘London omakase spotsã€theinfatuation.comã€‘ | 
| **Alâ€¯Forno** â€“ Wimbledon | Italian (brickâ€‘oven pizza & pasta) | Â£Â£Â£Â£ (expensive) | 4.7â€¯/â€¯4.7â€¯â­ (Infatuation) | Rustic brickâ€‘oven pizzas, solid pastas and a standout banoffee; great for groups who want a relaxed yet polished dinnerã€theinfatuation.comã€‘ | 
| **Grilandia â€“ Wimbledon** | Lebanese / Mediterranean | Â£Â£Â£ (moderate) | 4.7â€¯/â€¯4.7â€¯â­ (OT) | Allâ€‘day brasserie with modern Lebanese mezze, a bright interior and a solid 4.7 rating on OpenTableã€opentable.comã€‘ | 

\*Price bands are the OpenTable symbols:â€¯Â£â€¯=â€¯budget,â€¯Â£Â£â€¯=â€¯moderate,â€¯Â£Â£Â£â€¯=â€¯expensive,â€¯Â£Â£Â£Â£â€¯=â€¯veryâ€¯expensive.

---

## 2.  Casualâ€‘Bistro / Lunchâ€‘Andâ€‘Brunch Favorites  

| Restaurant | Cuisine | Price | Rating | Highlights |
|------------|---------|-------|--------|------------|
| **The Black Lamb** â€“ Wimbledon Village | Modern British, farmâ€‘toâ€‘fork | Â£Â£Â£ (expensive) | 4.6â€¯â­ (Map&Family) | Seasonal British set menus, a 5â€‘course Wimbledonâ€‘fortnight menu, and an onâ€‘site English wine listã€mapandfamily.comã€‘ |
| **Light House** â€“ Wimbledon Village | Contemporary European | Â£Â£ (moderate) | 4.5â€¯â­ (Map&Family) | Rotating seasonal dishes, liveâ€‘jazz evenings, and a relaxed garden terraceã€mapandfamily.comã€‘ |
| **Light on the Common** â€“ Wimbledon | Europeanâ€‘style cafÃ© | Â£Â£ (moderate) | 4.4â€¯â­ (Map&Family) | Breakfast smoothies, marketâ€‘fish stew, and a sunny conservatory for brunchã€mapandfamily.comã€‘ |
| **The Ivy CafÃ© Wimbledon** | Contemporary British | Â£Â£Â£Â£ (expensive) | 4.3â€¯â­ (OT) | Allâ€‘day menu, stylish interior, and a popular spot for families and business lunchesã€opentable.comã€‘ |
| **Patara Fine Thai** â€“ Wimbledon | Thai (modern) | Â£Â£Â£ (moderate) | 4.8â€¯â­ (Infatuation) | Outdoor garden terrace, standout green curry, and a menu that balances authentic Thai with a British twistã€theinfatuation.comã€‘ |
| **Thaiâ€¯Tho â€“ Wimbledon** | Thai (classic) | Â£Â£Â£ (moderate) | 4.7â€¯â­ (OT) | Familyâ€‘run, tennisâ€‘player favourite, generous portions of tomâ€¯yum and curriesã€opentable.comã€‘ |
| **Meganâ€™s in the Village** | Mediterraneanâ€‘inspired (Britishâ€‘Mediterranean) | Â£Â£ (moderate) | 4.5â€¯â­ (Map&Family) | Light, airy patio, brunch cocktails, and dogâ€‘friendly outdoor seatingã€mapandfamily.comã€‘ |
| **Centâ€¯Anni** â€“ Wimbledon Village | Italian | Â£Â£ (moderate) | 4.8â€¯â­ (Map&Family) | Handmade pasta, â€œPizza Mondayâ€ and â€œPasta Tuesdayâ€ specials, plus a kidsâ€™ menuã€mapandfamily.comã€‘ |

---

## 3.  Ethnic & Specialty Restaurants  

| Restaurant | Cuisine | Price | Rating | What makes it a â€œmustâ€‘tryâ€ |
|------------|---------|-------|--------|----------------------------|
| **Sticksâ€™nâ€™Sushi â€“ Wimbledon** | Japanese sushi & grilled sticks | Â£Â£Â£Â£ (expensive) | 4.6â€¯â­ (OT) / 4.9â€¯â­ (Infatuation) | Consistently excellent maki, creative toppings (e.g., tunaâ€‘BBQ sauce) and a sleek interior for date nightsã€theinfatuation.comã€‘ |
| **Bombayâ€¯Delight** â€“ Wimbledon | Indian (regional) | Â£Â£Â£ (moderate) | 4.7â€¯â­ (OT) / 4.7â€¯â­ (Infatuation) | Chefâ€‘driven menu with lesserâ€‘known regional dishes, no artificial colours, and a bright, celebratory vibeã€opentable.comã€‘ã€theinfatuation.comã€‘ |
| **Gigglingâ€¯Squid â€“ Wimbledon** | Thai â€œtapasâ€ | Â£Â£ (moderate) | 4.5â€¯â­ (TripAdvisor) | Smallâ€‘plate Thai streetâ€‘food style, vibrant dÃ©cor, and a popular spot for sharing with friendsã€tripadvisor.co.ukã€‘ |
| **Maisonâ€¯Stâ€¯Cassien** â€“ Wimbledon Village | Lebanese / Mediterranean | Â£Â£ (moderate) | 4.5â€¯â­ (Map&Family) | Authentic Lebanese mezze, full English breakfast, and a sunny courtyard that opens at 7â€¯amã€mapandfamily.comã€‘ |
| **Chango Empanadas** â€“ Wimbledon High St | Argentine (empanadas) | Â£Â£ (moderate) | 4.4â€¯â­ (Map&Family) | Handâ€‘rolled empanadas with creative fillings (pumpkinâ€‘goat cheese, spicy chicken) and a small wine list; perfect for a quick biteã€mapandfamily.comã€‘ |
| **Good Fortune Club** â€“ Wimbledon | Dim sum / Chinese | Â£Â£ (moderate) | 4.3â€¯â­ (Map&Family) | Newer dimâ€‘sum concept with a concise menu, good for a casual lunch or takeâ€‘awayã€mapandfamily.comã€‘ |
| **Rajdoot Tandoori** â€“ Wimbledon Village | Indian (classic) | Â£Â£ (moderate) | 4.2â€¯â­ (Map&Family) | Familyâ€‘run since 1982, reliable curries and a solid dinnerâ€‘time menuã€mapandfamily.comã€‘ |
| **Villageâ€¯Tandoori** â€“ Wimbledon | Indian (modern) | Â£Â£ (moderate) | 4.2â€¯â­ (Map&Family) | Recently refurbished, extensive menu, and a good option for lateâ€‘evening mealsã€mapandfamily.comã€‘ |

---

## 4.  Familyâ€‘Friendly / Pubâ€‘Style Options  

| Restaurant | Cuisine | Price | Rating | Why families love it |
|------------|---------|-------|--------|----------------------|
| **The Hand in Hand** â€“ Wimbledon Common | British pub | Â£Â£ (moderate) | 4.5â€¯â­ (Map&Family) | Classic Sunday roast, realâ€‘ale masterclasses, fireâ€‘place and outdoor garden â€“ great for kids and groupsã€mapandfamily.comã€‘ |
| **The Fox and Grapes** â€“ Wimbledon (also listed above) | British gastroâ€‘pub | Â£Â£Â£ (moderate) | 4.5â€¯â­ (OT) | Spacious patio, kidâ€‘friendly menu, and a relaxed vibe after a day at the courtsã€opentable.comã€‘ |
| **Billâ€™s Restaurant & Bar â€“ Wimbledon** | British (comfort) | Â£Â£ (moderate) | 4.4â€¯â­ (OT) | Large menu covering breakfast to dinner, vegetarian/vegan options, and a terrace for sunny daysã€opentable.comã€‘ |
| **The Alexandra** â€“ Wimbledon Hill Rd | British pub | Â£Â£ (moderate) | 4.4â€¯â­ (TripAdvisor) | Friendly service, solid pub classics, and a convenient location near the stationã€tripadvisor.co.ukã€‘ |
| **CÃ´te Brasserie â€“ Wimbledon** | French brasserie | Â£Â£Â£ (moderate) | 4.2â€¯â­ (OT) | Consistent French classics (steakâ€‘frites, croqueâ€‘monsieur) and a kidsâ€™ menu; good for a relaxed family dinnerã€opentable.comã€‘ |

---

## 5.  Quickâ€‘Bite / CafÃ© Picks (Great for a coffee break or light lunch)

| Spot | What they serve | Rating |
|------|----------------|--------|
| **Saucer & Cup** â€“ Wimbledon Park | Specialty coffee, pastries, light bites | 4.5â€¯â­ (TripAdvisor) |
| **Cakes by Robin** â€“ Wimbledon Stadium Business Park | Artisan cakes, coffee, glutenâ€‘free options | 4.5â€¯â­ (TripAdvisor) |
| **All Bar One â€“ Wimbledon** | International cafÃ©â€‘style menu, brunch plates | 4.3â€¯â­ (TripAdvisor) |
| **Francoâ€¯Manca â€“ Southfields** (near Wimbledon) | Sourdough pizza, simple salads | 4.2â€¯â­ (TripAdvisor) |
| **Meganâ€™s in the Village** (also listed above) | Light Mediterranean plates, brunch cocktails | 4.5â€¯â­ (Map&Family) |

---

## 6.  How to Choose & Book  

1. **For a special night out** â€“ go for **Buenosâ€¯Aires**, **Efesâ€¯Premium**, **Takahashi**, or **Alâ€¯Forno**.  
2. **For a relaxed lunch or brunch** â€“ try **The Black Lamb**, **Light House**, **Patara**, **Meganâ€™s**, or **Centâ€¯Anni**.  
3. **If youâ€™re after Asian flavors** â€“ **Sticksâ€™nâ€™Sushi**, **Gigglingâ€¯Squid**, **Bombayâ€¯Delight**, or **Thaiâ€¯Tho**.  
4. **Family or group meals** â€“ **The Hand in Hand**, **The Fox and Grapes**, **Billâ€™s**, or **CÃ´te Brasserie**.  
5. **Quick coffee or snack** â€“ **Saucer & Cup**, **Cakes by Robin**, or **All Bar One**.

All of the restaurants above are available for reservation on **OpenTable** (just search â€œWimbledonâ€ on the platform) and most have realâ€‘time availability, especially during the Wimbledon fortnight when bookings fill up fast.  

---

### Bottom Line

Wimbledonâ€™s dining scene is surprisingly diverse: you can enjoy worldâ€‘class Argentine steak, refined Japanese omakase, hearty British pub classics, vibrant Thai streetâ€‘food, and cozy Mediterranean mezze all within a 10â€‘minute walk of the tennis grounds.  Pick the vibe you want, check the price band, and book early (especially for the highâ€‘rated spots) â€“ youâ€™ll be set for a memorable meal whether youâ€™re celebrating a victory on Centre Court or just exploring the villageâ€™s charming streets.  



*All ratings are current as of the latest data (Mayâ€¯2025). Prices are shown in OpenTableâ€™s â€œÂ£â€ symbols; â€œÂ£Â£â€ = moderate, â€œÂ£Â£Â£â€ = expensive, â€œÂ£Â£Â£Â£â€ = very expensive.*
```

```
response.usage

ResponseUsage(input_tokens=52805, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=3083, output_tokens_details=OutputTokensDetails(reasoning_tokens=296), total_tokens=55888, cost=0.02, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_input_cost': 0, 'upstream_inference_output_cost': 0})
```

```
# You can also adjust context size: low, medium, high

response = client.responses.create(
    model="openai/gpt-oss-120b:free",
    tools=[
        {
            "type": "web_search_preview",
            "search_context_size": "low",
        }
    ],
    input="Who is the CEO of OpenAI?",
)

print(response.output_text)
```

> The chief executive officer of OpenAI is **Samâ€¯Altman**ã€openai.comã€‘. He has held the role since 2019 and continues to lead the companyâ€™s AI research and product development effortsã€britannica.comã€‘.

```
response.usage

ResponseUsage(input_tokens=46680, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=132, output_tokens_details=OutputTokensDetails(reasoning_tokens=77), total_tokens=46812, cost=0.02, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_input_cost': 0, 'upstream_inference_output_cost': 0})
```

#### Image Generation

```
import base64
import json
from openai import OpenAI

client = OpenAI(
    api_key="ollama",  # éšä¾¿å†™
    base_url="http://localhost:11434/v1"
)

response = client.responses.create(
    model="qwen3-vl:4b",
    input="Generate an image of cute coding dog wearing headset and glasses",
    tools=[{"type": "image_generation"}],
)

# Save the image to a file
image_data = [
    output.result
    for output in response.output
    if output.type == "image_generation_call"
]

if image_data:
    image_base64 = image_data[0]
    with open("coding_dog_new.png", "wb") as f:
        f.write(base64.b64decode(image_base64))
```
 
#### Follow up image instructions
 
```
 response_fwup = client.responses.create(
    model="gpt-4.1-mini",
    previous_response_id=response.id,
    input="Now make it wear sunglasses",
    tools=[{"type": "image_generation"}],
)

image_data_fwup = [
    output.result
    for output in response_fwup.output
    if output.type == "image_generation_call"
]

if image_data_fwup:
    image_base64 = image_data_fwup[0]
    with open("coding_dog_sunglasses.png", "wb") as f:
        f.write(base64.b64decode(image_base64))
```

#### Code Interpreter

```
instructions = """
You are a personal math tutor. When asked a math question, 
write and run code using the python tool to answer the question.
"""  # Keep "python tool" phrase in your prompt to ensure the model invokes this tool.

resp = client.responses.create(
    model="kwaipilot/kat-coder-pro:free",
    tools=[{"type": "code_interpreter", "container": {"type": "auto"}}],
    instructions=instructions,
    input="I need to solve the equation 3x + 11 = 14. Can you help me?",
)

print(resp.output)
```

```
[ResponseOutputMessage(id='msg_68776b41673481988ebba3edd1a6e939076deb4565eecd63', content=[ResponseOutputText(annotations=[], text="Sure! Let's solve the equation \\(3x + 11 = 14\\).\n\nI'll solve for \\(x\\).", type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), ResponseCodeInterpreterToolCall(id='ci_68776b4201c48198ad9f17586ff86dcf076deb4565eecd63', code="from sympy import symbols, Eq, solve\n\n# Define the variable\nx = symbols('x')\n\n# Define the equation\nequation = Eq(3*x + 11, 14)\n\n# Solve the equation\nsolution = solve(equation, x)\nsolution", container_id='cntr_68776b40d8748198a9e7352d828f636b0682ffa7a6b3eb87', outputs=None, status='completed', type='code_interpreter_call'), ResponseOutputMessage(id='msg_68776b4646188198b1bb6f7853bc3586076deb4565eecd63', content=[ResponseOutputText(annotations=[], text='The solution to the equation \\(3x + 11 = 14\\) is \\(x = 1\\).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]
```

```
for item in resp.output:
    if item.type == "code_interpreter_call":
        print("Code:\n", item.code, "\n")
    elif item.type == "message":
        for content_item in item.content:
            if content_item.type == "output_text":
                print("Text:\n", content_item.text, "\n")
```

```
Text:
 Sure! Let's solve the equation \(3x + 11 = 14\).

I'll solve for \(x\). 

Code:
 from sympy import symbols, Eq, solve

# Define the variable
x = symbols('x')

# Define the equation
equation = Eq(3*x + 11, 14)

# Solve the equation
solution = solve(equation, x)
solution 

Text:
 The solution to the equation \(3x + 11 = 14\) is \(x = 1\).
``` 
 

## åŸºäº File Search çš„çŸ¥è¯†å‹æ™ºèƒ½åŠ©æ‰‹å®è·µ

è¿‡å»ï¼Œæ„å»ºä¸€ä¸ªèƒ½å¤ŸåŸºäºç§æœ‰æ–‡ä»¶è¿›è¡Œé—®ç­”çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾€å¾€éœ€è¦å¼€å‘è€…è‡ªè¡Œå¤„ç†å‘é‡æ•°æ®åº“ã€è¯­ä¹‰æ£€ç´¢å’Œæ•°æ®å¬å›ç­‰å¤æ‚ç¯èŠ‚ï¼Œå·¥ç¨‹æˆæœ¬å’Œç»´æŠ¤æˆæœ¬éƒ½ç›¸å¯¹è¾ƒé«˜ã€‚è€Œå€ŸåŠ© **OpenAI çš„ File Search å·¥å…·ä¸ Responses API**ï¼Œè¿™ä¸€æµç¨‹è¢«æ˜¾è‘—ç®€åŒ–ï¼ŒçŸ¥è¯†å‹åŠ©æ‰‹çš„å¼€å‘é—¨æ§›å¤§å¹…é™ä½ã€‚

åœ¨å®è·µä¸­ï¼Œå¼€å‘æµç¨‹ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå°†ä¸šåŠ¡ç›¸å…³æ–‡æ¡£æŒ‰ç…§ OpenAI è§„èŒƒä¸Šä¼ ä¸ºæ–‡ä»¶èµ„æºï¼›å…¶æ¬¡ï¼Œåˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆVector Storeï¼‰ï¼Œå¹¶å°†æ–‡ä»¶åŠ å…¥å…¶ä¸­ä»¥å®Œæˆè¯­ä¹‰å‘é‡åŒ–ï¼›æœ€åï¼Œåœ¨ Responses API ä¸­å¯ç”¨ `file_search` å·¥å…·ï¼ŒåŸºäºå‘é‡å­˜å‚¨å®ç°é«˜æ•ˆã€å‡†ç¡®çš„è¯­ä¹‰æ£€ç´¢ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€å¼€å‘è€…è‡ªè¡Œç»´æŠ¤å‘é‡æ•°æ®åº“æˆ–æœç´¢é€»è¾‘ï¼Œæå¤§æå‡äº†å¼€å‘æ•ˆç‡ã€‚

åŸºäºè¿™ä¸€èƒ½åŠ›ï¼Œå¯ä»¥å¿«é€Ÿæ„å»ºä¸€ä¸ªç»ˆç«¯æˆ– Web å½¢å¼çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œç”¨äºå›ç­”ç”¨æˆ·å…³äºæ´»åŠ¨å®‰æ’ã€è§„åˆ™è¯´æ˜ç­‰é—®é¢˜ã€‚å®é™…æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆä¸ä»…å“åº”é€Ÿåº¦å¿«ï¼ˆé€šå¸¸åœ¨æ•°ç§’å†…å®Œæˆï¼‰ï¼Œè€Œä¸”é€šè¿‡é™åˆ¶æ£€ç´¢ç»“æœæ•°é‡ï¼Œæœ‰æ•ˆæ§åˆ¶äº† Token æ¶ˆè€—ï¼Œå…·å¤‡è‰¯å¥½çš„æˆæœ¬æ•ˆç‡ã€‚

> File Search å°†â€œçŸ¥è¯†æ³¨å…¥â€ä»å¤æ‚çš„å·¥ç¨‹é—®é¢˜ï¼Œè½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„ API èƒ½åŠ›ï¼Œæ˜¯æ„å»ºä¼ä¸šçº§çŸ¥è¯†é—®ç­”ä¸å†…éƒ¨æ™ºèƒ½åŠ©æ‰‹çš„å…³é”®åŸºç¡€è®¾æ–½ã€‚


                                         
![Alt Image Text](../images/chap8_0_3.png "Body image")  


**`upload_file.ipynb`**

```
import json
from openai import OpenAI

with open("../config.json") as f:
    config = json.load(f)

# Access API key
api_key = config["openai_api_key"]

client = OpenAI(api_key=api_key)
```

#### Create the file

```
import requests
from io import BytesIO

def create_file(client, file_path):
    if file_path.startswith("http://") or file_path.startswith("https://"):
        # Download the file content from the URL
        response = requests.get(file_path)
        file_content = BytesIO(response.content)
        file_name = file_path.split("/")[-1]
        file_tuple = (file_name, file_content)
        result = client.files.create(file=file_tuple, purpose="assistants")
    else:
        # Handle local file path
        with open(file_path, "rb") as file_content:
            result = client.files.create(file=file_content, purpose="assistants")
    print(result.id)
    return result.id


# Replace with your own file path or URL
file_id = create_file(client, "AI_Summit_Program.pdf")
```

#### Create a vector store

```
vector_store = client.vector_stores.create(name="knowledge_base")
print(vector_store.id)
```

#### Add the file to the vector store

```
client.vector_stores.files.create(vector_store_id=vector_store.id, file_id=file_id)
```

> VectorStoreFile(id='file-ABB2cqjqiMisJchkZfY48q', created_at=1752671502, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_687782eca894819185cc116786181154', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))


#### Check status

```
result = client.vector_stores.files.list(vector_store_id=vector_store.id)
print(result)
```

> SyncCursorPage[VectorStoreFile](data=[VectorStoreFile(id='file-ABB2cqjqiMisJchkZfY48q', created_at=1752671502, last_error=None, object='vector_store.file', status='completed', usage_bytes=3091, vector_store_id='vs_687782eca894819185cc116786181154', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static')), VectorStoreFile(id='file-9XcTuxiJTray6ttcpRf1Kw', created_at=1752663761, last_error=None, object='vector_store.file', status='completed', usage_bytes=3118, vector_store_id='vs_687782eca894819185cc116786181154', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))], has_more=False, object='list', first_id='file-ABB2cqjqiMisJchkZfY48q', last_id='file-9XcTuxiJTray6ttcpRf1Kw')


## What Is the OpenAI Agents SDK?

Agents are AI assistants that can actually perform tasks autonomously.

### **Building Agents**

**Models**: o1, 03-mini, GPT-4.5, GPT-4o, GPT-4o-mini

**Tools**: Function calling, web search, computer use

**Knowledge and memory**: Vector stores, file search, embeddings

**Guardrails**: Moderation, instruction hierarchy (Python, TypeScript)

**Orchestration**: Agents SDK, tracing, evaluations, fine-tuning


**Main Building Blocks of Agents SDK**


### å†…å®¹æ€»ç»“ï¼ˆæ¦‚è¿°ï¼‰

æœ¬æ¨¡å—ç³»ç»Ÿæ€§ä»‹ç»äº† **OpenAI Agents SDK** åŠå…¶åœ¨æ„å»ºè‡ªæ²»å‹æ™ºèƒ½ä½“ï¼ˆAgentic Applicationsï¼‰ä¸­çš„æ ¸å¿ƒä»·å€¼ã€‚

æ™ºèƒ½ä½“ä¸å†åªæ˜¯è¢«åŠ¨å›ç­”é—®é¢˜çš„æ¨¡å‹ï¼Œè€Œæ˜¯èƒ½å¤ŸåŸºäº**æ¨ç†ã€å·¥å…·è°ƒç”¨ã€çŸ¥è¯†ä¸è®°å¿†**è‡ªä¸»å®Œæˆä»»åŠ¡çš„ AI åŠ©æ‰‹ã€‚OpenAI å°†æ™ºèƒ½ä½“çš„æ„å»ºæ‹†è§£ä¸ºå¤šä¸ªå…³é”®é¢†åŸŸï¼šæ¨¡å‹ï¼ˆModelsï¼‰ã€å·¥å…·ï¼ˆToolsï¼‰ã€çŸ¥è¯†ä¸è®°å¿†ï¼ˆKnowledge & Memoryï¼‰ã€å®‰å…¨é˜²æŠ¤ï¼ˆGuardrailsï¼‰ä»¥åŠç¼–æ’ä¸ç®¡ç†ï¼ˆOrchestrationï¼‰ã€‚

Agents SDK æ˜¯ä¸€ä¸ªè½»é‡ä½†åŠŸèƒ½å¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºå°†ä¸Šè¿°èƒ½åŠ›é«˜æ•ˆç»„åˆèµ·æ¥ï¼Œç®€åŒ–å¤æ‚çš„æ™ºèƒ½ä½“å·¥ä½œæµã€‚SDK æä¾›å››ä¸ªæ ¸å¿ƒæ„å»ºæ¨¡å—ï¼š

* **Agents**ï¼šå…·å¤‡æŒ‡ä»¤ä¸å·¥å…·çš„ä¸“ä¸šåŒ– AI å·¥ä½œå•å…ƒ
* **Handoffs**ï¼šæ”¯æŒä»»åŠ¡åœ¨å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„å§”æ´¾ä¸åä½œ
* **Guardrails**ï¼šç”¨äºè¾“å…¥æ ¡éªŒä¸å®‰å…¨æ§åˆ¶ï¼Œé˜²æ­¢å¼‚å¸¸æˆ–ä¸å½“è¡Œä¸º
* **Sessions**ï¼šè‡ªåŠ¨ç»´æŠ¤è·¨è½®æ¬¡å¯¹è¯çŠ¶æ€ï¼Œå…å»æ‰‹åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†

æ­¤å¤–ï¼ŒSDK å†…å»ºå®Œæ•´çš„ **Agent Loop**ï¼Œè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨ä¸å¤šæ­¥æ¨ç†æµç¨‹ï¼Œå¹¶åŸç”Ÿæ”¯æŒ Pythonï¼Œä½¿å¼€å‘è€…æ— éœ€å¼•å…¥å¤æ‚æŠ½è±¡å³å¯å¿«é€Ÿæ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚åŒæ—¶ï¼Œå†…ç½®çš„ **Tracing èƒ½åŠ›**å¯ç”¨äºè°ƒè¯•ã€ç›‘æ§ä¸è¯„ä¼°æ™ºèƒ½ä½“è¡Œä¸ºï¼Œå¹¶ä¸ OpenAI å¹³å°çš„è¯„æµ‹ä¸ä¼˜åŒ–å·¥å…·æ·±åº¦é›†æˆã€‚


> OpenAI Agents SDK ä¸ºæ„å»ºè‡ªæ²»å‹ AI ç³»ç»Ÿæä¾›äº†ä¸€ç§å…¼é¡¾ç®€æ´æ€§ä¸æ‰©å±•æ€§çš„å·¥ç¨‹åŒ–æ–¹æ¡ˆã€‚é€šè¿‡å°†æ¨¡å‹èƒ½åŠ›ã€å·¥å…·è°ƒç”¨ã€è®°å¿†ç®¡ç†ä¸å®‰å…¨æœºåˆ¶è¿›è¡Œæ¨¡å—åŒ–æ•´åˆï¼Œå¼€å‘è€…å¯ä»¥ä»å•ä¸€æ™ºèƒ½ä½“å¿«é€Ÿæ¼”è¿›åˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“åä½œä½“ç³»ã€‚å…¶å†…ç½®çš„ Agent Loopã€è‡ªåŠ¨ä¼šè¯ç®¡ç†ä¸å¯è§‚æµ‹æ€§è®¾è®¡ï¼Œå¤§å¹…é™ä½äº† Agentic Workflow çš„å®ç°é—¨æ§›ï¼Œä½¿æ™ºèƒ½ä½“åº”ç”¨çœŸæ­£å…·å¤‡å¯è½åœ°ã€å¯ç»´æŠ¤ä¸å¯æ‰©å±•çš„å·¥ç¨‹ä»·å€¼ã€‚


![Alt Image Text](../images/chap8_0_4.png "Body image")  


ä»¥ä¸‹è¿™æ®µä»£ç å±•ç¤ºäº†å¦‚ä½•ç”¨ Agents SDK æ„å»ºä¸€ä¸ªå…·å¤‡ã€Œè¾“å…¥å®¡æŸ¥ + æ„å›¾ç†è§£ + ä¸“å®¶è·¯ç”±ã€èƒ½åŠ›çš„æœ¬åœ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¹¶é€šè¿‡ Ollama å®ç°å®Œå…¨ç¦»çº¿è¿è¡Œã€‚

å®ƒä½“ç°äº†ä» Prompt Engineering å‘ Agent System Engineering çš„å…³é”®è½¬å˜ã€‚

**`first_agents_local.py`**

```
from agents import (
    Agent,
    InputGuardrail,
    GuardrailFunctionOutput,
    Runner,
)
from pydantic import BaseModel
import asyncio
import os
from agents import Agent, Runner

# ---------------------------------------------------------------------
# 0. Ollama OpenAI-compatible configuration
# ---------------------------------------------------------------------

# Ollama ä½¿ç”¨ OpenAI-compatible API
os.environ["OPENAI_API_KEY"] = "ollama"  # ä»»æ„å€¼å³å¯
os.environ["OPENAI_BASE_URL"] = "http://localhost:11434/v1"
# å…³é—­ Agents SDK Tracingï¼ˆæœ¬åœ° Ollama å¿…é¡»ï¼‰
os.environ["OPENAI_TRACING"] = "false"

MODEL_NAME = "qwen2.5:7b"   # æˆ– qwen2.5 / deepseek-r1 ç­‰

# ---------------------------------------------------------------------
# 1. Guardrail: â€œIs this a coding question?â€
# ---------------------------------------------------------------------

class CodeQuestionOutput(BaseModel):
    is_code_question: bool
    reasoning: str


guardrail_agent = Agent(
    name="Guardrail check",
    model=MODEL_NAME,
    instructions="Check if the user is asking about programming.",
    output_type=CodeQuestionOutput,
)


async def code_question_guardrail(ctx, agent, input_data):
    result = await Runner.run(
        guardrail_agent,
        input_data,
        context=ctx.context,
    )
    final_output = result.final_output_as(CodeQuestionOutput)
    return GuardrailFunctionOutput(
        output_info=final_output,
        tripwire_triggered=not final_output.is_code_question,
    )


# ---------------------------------------------------------------------
# 2. Specialist coding agents
# ---------------------------------------------------------------------

python_tutor_agent = Agent(
    name="Python Tutor",
    model=MODEL_NAME,
    handoff_description="Answers Python-related programming questions",
    instructions=(
        "You are an expert Python mentor. "
        "Explain concepts clearly and give concise code examples."
    ),
)

javascript_tutor_agent = Agent(
    name="JavaScript Tutor",
    model=MODEL_NAME,
    handoff_description="Answers JavaScript / TypeScript coding questions",
    instructions=(
        "You are an expert JavaScript mentor. "
        "Explain concepts clearly and give concise code examples."
    ),
)

# ---------------------------------------------------------------------
# 3. Triage agent (Manager agent)
# ---------------------------------------------------------------------

triage_agent = Agent(
    name="Triage Agent",
    model=MODEL_NAME,
    instructions=(
        "Decide which tutor agent should answer the user's programming question."
    ),
    handoffs=[python_tutor_agent, javascript_tutor_agent],
    input_guardrails=[
        InputGuardrail(guardrail_function=code_question_guardrail),
    ],
)

# ---------------------------------------------------------------------
# 4. Demo
# ---------------------------------------------------------------------

async def main():
    result = await Runner.run(
        triage_agent,
        "How do I set 1 second delay in JavaScript? Explain shortly.",
    )
    print(result.final_output)

    print("\n" + "-" * 40 + "\n")

    result = await Runner.run(
        triage_agent,
        "How to write a for loop in Python? Explain shortly.",
    )
    print(result.final_output)

    print("\n" + "-" * 40 + "\n")

    result = await Runner.run(
        triage_agent,
        "Who won the World Cup in 1998?",
    )
    print(result.final_output)


if __name__ == "__main__":
    asyncio.run(main())
```

#### Output

```
% python ./first_agents_local.py
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: max retries reached, giving up on this batch.
[non-fatal] Tracing: request failed: timed out
To set a 1-second delay in JavaScript, you can use the `setTimeout` function. Hereâ€™s a concise example:

javascript
setTimeout(() => {
    console.log("This message will be printed after 1 second.");
}, 1000);


In this code:
- `setTimeout` is called with two arguments: 
  - A function that you want to run after the delay.
  - The duration of the delay in milliseconds (1000 milliseconds = 1 second).

This will log "This message will be printed after 1 second." to the console after a 1-second delay.

----------------------------------------

[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: max retries reached, giving up on this batch.
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: max retries reached, giving up on this batch.
[non-fatal] Tracing: request failed: timed out
1
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: request failed: timed out
[non-fatal] Tracing: max retries reached, giving up on this batch.
Sure! Let's dive into how to write a for loop in Python.

### For Loop Basics

A `for` loop is used when you want to iterate over a sequence (such as a list, tuple, dictionary, set, or string) or any other iterable object. Hereâ€™s a simple example:

python
# Example: Print each number from 1 to 5
numbers = [1, 2, 3, 4, 5]
for number in numbers:
    print(number)


### Looping with Ranges

You can also use the `range` function to generate a sequence of numbers. The `range` function returns an iterator that generates numbers from start (inclusive) to stop (exclusive).

python
# Example: Print numbers from 1 to 5 using range
for i in range(1, 6):
    print(i)


### Looping with Dictionaries

When you want to iterate over the keys, values, or both of a dictionary.

#### Iterating Over Keys:

python
# Example: Print each key in a dictionary
fruits = {"apple": "red", "banana": "yellow", "cherry": "red"}
for fruit in fruits:
    print(fruit)


#### Iterating Over Key-Value Pairs:

python
# Example: Print each key-value pair in a dictionary
fruits = {"apple": "red", "banana": "yellow", "cherry": "red"}
for fruit, color in fruits.items():
    print(f"{fruit} is {color}")


#### Iterating Over Values:

python
# Example: Print the value for each key in a dictionary
fruits = {"apple": "red", "banana": "yellow", "cherry": "red"}
for color in fruits.values():
    print(color)

### Looping with Strings

You can also use `for` loops to iterate over characters in a string.

python
# Example: Print each character in a string
message = "Hello, world!"
for char in message:
    print(char)


Feel free to ask if you have any specific scenarios or further questions about for loops!

----------------------------------------

[non-fatal] Tracing: request failed: timed out
Traceback (most recent call last):
  File "/Users/jacob/learnspace/ai/agent/openai-responses-api-and-agents-sdk/module_2/./first_agents_local.py", line 120, in <module>
    asyncio.run(main())
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/Users/jacob/learnspace/ai/agent/openai-responses-api-and-agents-sdk/module_2/./first_agents_local.py", line 112, in main
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py", line 367, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py", line 664, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py", line 1776, in _run_input_guardrails
    raise InputGuardrailTripwireTriggered(result)
agents.exceptions.InputGuardrailTripwireTriggered: Guardrail InputGuardrail triggered tripwire
```


ä¸‹é¢æˆ‘**ç«™åœ¨ã€ŒAgent / å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡ä¸“å®¶ã€çš„è§†è§’**ï¼Œå¯¹ä½ è¿™æ®µä»£ç åšä¸€æ¬¡**ç»“æ„åŒ–ã€é€å±‚æ‹†è§£çš„ä¸“ä¸šåˆ†æ**ã€‚æˆ‘ä¼šä¸ä»…è§£é‡Šã€Œå®ƒåšäº†ä»€ä¹ˆã€ï¼Œè¿˜ä¼šè¯´æ˜ã€Œä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡ã€ã€Œè®¾è®¡ä¸Šçš„ä¼˜ç‚¹ / éšæ‚£ã€ã€Œåœ¨çœŸå®ç³»ç»Ÿä¸­å¦‚ä½•æ¼”è¿›ã€ã€‚



### ä¸€ã€æ•´ä½“æ¶æ„æ€»è§ˆï¼ˆä»ç³»ç»Ÿè§†è§’ï¼‰

è¿™æ®µä»£ç å®ç°çš„æ˜¯ä¸€ä¸ª**ç»å…¸çš„ä¸‰å±‚ Agent æ¶æ„**ï¼š

```
User Input
   â”‚
   â–¼
Input Guardrailï¼ˆè¾“å…¥å®‰å…¨/æ„å›¾è¿‡æ»¤ï¼‰
   â”‚
   â–¼
Triage / Manager Agentï¼ˆè·¯ç”±ä¸å†³ç­–ï¼‰
   â”‚
   â”œâ”€â”€ Python Specialist Agent
   â””â”€â”€ JavaScript Specialist Agent
```

**æ ¸å¿ƒç›®æ ‡**ï¼š

> æ„å»ºä¸€ä¸ªã€Œèƒ½ç†è§£æ„å›¾ â†’ æ ¡éªŒåˆæ³•æ€§ â†’ è‡ªåŠ¨è·¯ç”±åˆ°ä¸“å®¶ã€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

è¿™ä¸æ˜¯èŠå¤©æœºå™¨äººï¼Œè€Œæ˜¯ä¸€ä¸ª**è‡ªæ²»ï¼ˆautonomousï¼‰æ¨ç†ç³»ç»Ÿ**ã€‚

---

### äºŒã€æ ¸å¿ƒæ¦‚å¿µé€ä¸€æ‹†è§£

#### 1ï¸âƒ£ Agentï¼šä¸æ˜¯ Promptï¼Œè€Œæ˜¯ã€Œè§’è‰² + èƒ½åŠ› + è¾¹ç•Œã€

ä»¥ `python_tutor_agent` ä¸ºä¾‹ï¼š

```python
python_tutor_agent = Agent(
    name="Python Tutor",
    model=MODEL_NAME,
    handoff_description="Answers Python-related programming questions",
    instructions="You are an expert Python mentor..."
)
```

**ä» Agent è®¾è®¡è§’åº¦çœ‹ï¼Œå®ƒåŒ…å« 4 ä¸ªç»´åº¦ï¼š**

| ç»´åº¦                    | å«ä¹‰                           |
| --------------------- | ---------------------------- |
| `name`                | ç”¨äºæ¨ç†ä¸ tracing çš„**èº«ä»½æ ‡è¯†**      |
| `instructions`        | å†³å®š agent çš„**æ€ç»´ä¸è¾“å‡ºé£æ ¼**        |
| `handoff_description` | ç»™ **Manager Agent** ç”¨çš„â€œèƒ½åŠ›è¯´æ˜â€ |
| `model`               | agent çš„**è®¤çŸ¥èƒ½åŠ›ä¸Šé™**            |

> âš ï¸ æ³¨æ„ï¼š
> `handoff_description` **ä¸æ˜¯ç»™ç”¨æˆ·çœ‹çš„**ï¼Œè€Œæ˜¯ç»™ *å¦ä¸€ä¸ª LLM* çœ‹çš„ã€‚

è¿™æ„å‘³ç€ä½ å·²ç»åœ¨åšï¼š

> **LLM â†’ LLM çš„èƒ½åŠ›åå•†ï¼ˆcapability negotiationï¼‰**

### 2ï¸âƒ£ Guardrail Agentï¼šæŠŠâ€œå®‰å…¨â€ä»ä¸šåŠ¡é€»è¾‘ä¸­å‰¥ç¦»

```python
guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the user is asking about programming.",
    output_type=CodeQuestionOutput,
)
```

**è¿™æ˜¯ä¸€ä¸ª**ä¸“ç”¨åˆ¤å®š Agent**ï¼Œç‰¹ç‚¹ï¼š**

* åªåšä¸€ä»¶äº‹ï¼š**åˆ†ç±» / åˆ¤å®š**
* ä½¿ç”¨ `output_type` å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º
* ä¸ä¸»ä¸šåŠ¡ Agent å®Œå…¨è§£è€¦

**Guardrail çš„è¿è¡Œæ–¹å¼**

```python
async def code_question_guardrail(ctx, agent, input_data):
    result = await Runner.run(guardrail_agent, input_data)
```

ä½ è¿™é‡Œåšäº†ä¸€ä»¶éå¸¸ä¸“ä¸šçš„äº‹ï¼š

> **ç”¨ Agent æœ¬èº«ä½œä¸º Guardrailï¼Œè€Œä¸æ˜¯å†™ if/else**

**å¥½å¤„ï¼š**

| ä¼ ç»Ÿ if/else | Agent Guardrail |
| ---------- | --------------- |
| è§„åˆ™æ­»æ¿       | è¯­ä¹‰ç†è§£            |
| ç»´æŠ¤æˆæœ¬é«˜      | å¯æ¼”è¿›             |
| éš¾ä»¥å¤šè¯­è¨€      | å¤©ç„¶å¤šè¯­è¨€           |


#### 3ï¸âƒ£ GuardrailFunctionOutputï¼šæ§åˆ¶æµâ€œé—¸é—¨â€

```python
return GuardrailFunctionOutput(
    output_info=final_output,
    tripwire_triggered=not final_output.is_code_question,
)
```

è¿™æ˜¯**ç³»ç»Ÿçº§æ§åˆ¶ç‚¹**ï¼š

* `tripwire_triggered = True`

  * âŒ ç»ˆæ­¢ Agent æµç¨‹
* `False`

  * âœ… æ”¾è¡Œç»™ä¸‹æ¸¸ Agent

ğŸ‘‰ **æ³¨æ„ä¸€ä¸ªå…³é”®è®¾è®¡ç‚¹**ï¼š

> Guardrail å¹¶ä¸â€œå›ç­”ç”¨æˆ·â€ï¼Œ
> å®ƒåª**å†³å®šæ˜¯å¦å…è®¸ç³»ç»Ÿç»§ç»­æ€è€ƒ**ã€‚

è¿™å’Œ Web å®‰å…¨ä¸­çš„ **WAF / API Gateway** æ˜¯åŒä¸€æ€æƒ³ã€‚


#### 4ï¸âƒ£ Triage Agentï¼šçœŸæ­£çš„â€œAgent Managerâ€

```python
triage_agent = Agent(
    name="Triage Agent",
    instructions="You determine which agent to use...",
    handoffs=[python_tutor_agent, javascript_tutor_agent],
)
```

è¿™æ˜¯æ•´å¥—ç³»ç»Ÿçš„**å¤§è„‘**ã€‚

**å®ƒçš„èŒè´£ä¸æ˜¯å›ç­”é—®é¢˜ï¼Œè€Œæ˜¯ï¼š**

1. ç†è§£ç”¨æˆ·æ„å›¾
2. æ¯”è¾ƒ handoffs ä¸­çš„èƒ½åŠ›æè¿°
3. é€‰æ‹©æœ€åˆé€‚çš„ Agent
4. æŠŠä¸Šä¸‹æ–‡äº¤æ¥è¿‡å»

**è¿™æ˜¯å…¸å‹çš„ **LLM-driven routing****

ä¸ä¼ ç»Ÿæ¶æ„å¯¹æ¯”ï¼š

| ä¼ ç»Ÿç³»ç»Ÿ        | Agent ç³»ç»Ÿ    |
| ----------- | ----------- |
| switch / if | è¯­ä¹‰æ¨ç†        |
| ç¡¬ç¼–ç è·¯ç”±       | åŠ¨æ€èƒ½åŠ›åŒ¹é…      |
| æ‰©å±•æˆæœ¬é«˜       | æ–°å¢ Agent å³å¯ |


#### 5ï¸âƒ£ Runner.runï¼šä¸€æ¬¡â€œå—æ§è‡ªæ²»æ‰§è¡Œâ€

```python
result = await Runner.run(triage_agent, input)
```

è¿™ä¸€è¡Œå®é™…ä¸Šè§¦å‘äº†ä¸€ä¸ª**å®Œæ•´çš„è‡ªæ²»æ¨ç†å¾ªç¯**ï¼š

1. æ‰§è¡Œ input guardrails
2. è°ƒç”¨ triage agent
3. åˆ¤æ–­æ˜¯å¦éœ€è¦ handoff
4. æ‰§è¡Œè¢«é€‰ä¸­çš„ agent
5. èšåˆæœ€ç»ˆè¾“å‡º

> ä½ å¹¶æ²¡æœ‰å†™æµç¨‹æ§åˆ¶ä»£ç 
> **æµç¨‹ç”± Agent è‡ªå·±æ¨å¯¼å‡ºæ¥**

è¿™æ­£æ˜¯ Agents SDK çš„æ ¸å¿ƒä»·å€¼ã€‚



### ä¸‰ã€ä»ç³»ç»Ÿå·¥ç¨‹è§’åº¦çš„è¯„ä»·


#### 1ï¸âƒ£ èŒè´£åˆ†ç¦»æå…¶æ¸…æ™°

* åˆ¤å®š â‰  è·¯ç”± â‰  æ‰§è¡Œ
* æ¯ä¸ª Agent éƒ½æ˜¯ **Single Responsibility**

#### 2ï¸âƒ£ å¯æ‰©å±•æ€§æå¼º

ä½ ç°åœ¨å¯ä»¥è½»æ¾åŠ ï¼š

```python
go_tutor_agent
rust_tutor_agent
sql_tutor_agent
```

**æ— éœ€ä¿®æ”¹ä»»ä½•å·²æœ‰é€»è¾‘**

#### 3ï¸âƒ£ çœŸå®ç”Ÿäº§çº§æ€è·¯

* Guardrail
* Manager
* Specialist

è¿™æ˜¯ **AutoGPT / Devin / Copilot å†…éƒ¨å¸¸è§ç»“æ„**



### âš ï¸ æ½œåœ¨é—®é¢˜ï¼ˆä¸“å®¶è§†è§’ï¼‰

#### 1ï¸âƒ£ æ¨ç†æˆæœ¬åé«˜ï¼ˆæœ¬åœ°æ¨¡å‹å°¤å…¶æ˜æ˜¾ï¼‰

ä¸€æ¬¡è¯·æ±‚ â‰ˆ 3 æ¬¡ LLM è°ƒç”¨

ğŸ‘‰ ä¼˜åŒ–æ–¹å‘ï¼š

* åˆå¹¶ Guardrail + Triage
* æˆ– Guardrail ç”¨ 3B æ¨¡å‹



#### 2ï¸âƒ£ Guardrail çš„ JSON ç¨³å®šæ€§

æœ¬åœ° Ollama æ¨¡å‹å¯èƒ½ï¼š

* æ¼å­—æ®µ
* è¾“å‡ºè‡ªç„¶è¯­è¨€

ğŸ‘‰ ç”Ÿäº§ä¸­å»ºè®®ï¼š

* åœ¨ `instructions` ä¸­å¼ºåˆ¶ JSON
* æˆ– fallback retry


#### 3ï¸âƒ£ Triage Agent çš„ instructions åå¼±

å½“å‰ï¼š

```text
You determine which agent to use
```

åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œå¯èƒ½ï¼š

* è·¯ç”±ä¸ç¨³å®š
* è¯¯é€‰ Agent

ğŸ‘‰ ä¸“å®¶å»ºè®®ï¼š

```text
Choose exactly one agent whose handoff_description best matches the user's intent.
Do not answer the question yourself.
```


## å¯é…ç½®åŒ–æ™ºèƒ½ä½“ï¼šOpenAI Agents SDK çš„å·¥ç¨‹èƒ½åŠ›è§£æ

OpenAI Agents SDK æä¾›äº†é«˜åº¦å¯é…ç½®çš„æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å®šä¹‰æ–¹å¼ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ ¹æ®ä¸šåŠ¡åœºæ™¯çµæ´»å¡‘é€  Agent çš„è¡Œä¸ºã€èƒ½åŠ›ä¸è¾“å‡ºå½¢å¼ã€‚é€šè¿‡ä¸€ç»„æ¸…æ™°çš„é…ç½®é¡¹ï¼ŒAgent ä¸å†æ˜¯å›ºå®šé€»è¾‘çš„å¯¹è¯æ¨¡å‹ï¼Œè€Œæ˜¯å¯æ§ã€å¯ç»„åˆçš„æ‰§è¡Œå•å…ƒã€‚

åœ¨åŸºç¡€é…ç½®å±‚é¢ï¼Œæ¯ä¸ª Agent éƒ½éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„ **Name**ï¼Œç”¨äºæ—¥å¿—ã€Tracing å’Œè¿è¡Œç›‘æ§ã€‚**Instructions** ç›¸å½“äºç³»ç»Ÿçº§æŒ‡ä»¤ï¼Œç”¨äºå®šä¹‰ Agent çš„èŒè´£ä¸è¡Œä¸ºå‡†åˆ™ï¼›è€Œ **Model** åˆ™æ˜ç¡®æŒ‡å®šæ‰€ä½¿ç”¨çš„ LLMã€‚ä¸ä¼ ç»Ÿæ¥å£ä¸åŒï¼ŒAgent å¯ä»¥ç›´æ¥å°†è‡ªå®šä¹‰å‡½æ•°ä½œä¸º **Tools** æŒ‚è½½ï¼Œä»è€Œå®ç°æ›´ç´§å¯†çš„ä¸šåŠ¡èƒ½åŠ›é›†æˆã€‚

**Context** æ˜¯ Agents SDK ä¸­æå…·å·¥ç¨‹ä»·å€¼çš„ä¸€é¡¹èƒ½åŠ›ï¼Œç”¨äºåœ¨ Agentã€å·¥å…·ä»¥åŠå¤šæ¬¡ Handoff ä¹‹é—´å…±äº«ç»“æ„åŒ–ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·èº«ä»½ã€è®¢é˜…çŠ¶æ€æˆ–ä¸šåŠ¡ ID éƒ½å¯ä»¥é€šè¿‡ Context æ³¨å…¥ï¼Œå¹¶åœ¨æ•´ä¸ªå·¥ä½œæµä¸­ä¿æŒä¸€è‡´ã€‚è¿™ç§è®¾è®¡é¿å…äº†é‡å¤æŸ¥è¯¢ä¸çŠ¶æ€åŒæ­¥é—®é¢˜ï¼Œä½¿å¤š Agent åä½œæ›´åŠ ç¨³å®šã€å¯é¢„æµ‹ã€‚

åœ¨è¾“å‡ºæ§åˆ¶æ–¹é¢ï¼ŒSDK æ”¯æŒä¸º Agent å®šä¹‰ä¸¥æ ¼çš„ **Output Schema**ï¼Œä¾‹å¦‚åªå…è®¸è¿”å›ç‰¹å®šçš„æ•°æ®ç»“æ„ã€‚

**è¿™ä¸€èƒ½åŠ›å¯¹äºå¤š Agent ç¼–æ’å°¤ä¸ºå…³é”®ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è¾“å‡ºæ­§ä¹‰ä¸æ¨¡å‹å¹»è§‰ï¼Œæå‡ç³»ç»Ÿæ•´ä½“çš„å¯é æ€§ã€‚**

æ­¤å¤–ï¼ŒAgent çš„ **Instructions** è¿˜å¯ä»¥æ ¹æ®è¿è¡Œæ—¶æ¡ä»¶åŠ¨æ€ç”Ÿæˆï¼Œä½¿å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹å±•ç°ä¸åŒç­–ç•¥ï¼Œä»è€Œæ„å»ºæ›´å…·å¼¹æ€§çš„æ™ºèƒ½ä½“è¡Œä¸ºæ¨¡å‹ã€‚

æœ€åï¼Œé€šè¿‡ **Tool Use Settings**ï¼Œå¼€å‘è€…å¯ä»¥ç²¾ç¡®æ§åˆ¶å·¥å…·è°ƒç”¨ç­–ç•¥â€”â€”ä»å®Œå…¨è‡ªåŠ¨å†³ç­–ï¼Œåˆ°å¼ºåˆ¶è°ƒç”¨æŒ‡å®šå·¥å…·ï¼Œç¡®ä¿ Agent è¡Œä¸ºç¬¦åˆä¸šåŠ¡é¢„æœŸã€‚

> é€šè¿‡é«˜åº¦ç»“æ„åŒ–çš„é…ç½®èƒ½åŠ›ï¼ŒOpenAI Agents SDK å°†æ™ºèƒ½ä½“ä»â€œä¸å¯æ§çš„å¯¹è¯æ¨¡å‹â€è½¬å˜ä¸ºâ€œå¯ç¼–æ’ã€å¯çº¦æŸã€å¯æ¼”è¿›çš„å·¥ç¨‹ç»„ä»¶â€ï¼Œä¸ºæ„å»ºå¤æ‚ Agent ç³»ç»Ÿæä¾›äº†åšå®åŸºç¡€ã€‚

### Basic Configuration

ä¸‹é¢æ˜¯**å›¾ç‰‡ä¸­ä»£ç çš„å®Œæ•´æ–‡æœ¬ç‰ˆï¼ˆå·²é€è¡Œè¿˜åŸï¼Œå¯ç›´æ¥å¤åˆ¶è¿è¡Œï¼‰**ï¼š

```python
from agents import Agent, ModelSettings, function_tool


@function_tool
def get_favorite_food(animal: str) -> str:
    return f"{animal.capitalize()}'s favorite food is something tasty."


agent = Agent(
    name="FunFactsAgent",
    instructions="Always answer in a short and fun sentence for kids.",
    model="o3-mini",
    tools=[get_favorite_food],
)
```

**ç®€è¦è¯´æ˜ï¼ˆä¾¿äºä½ å†™åšå®¢æ—¶ç”¨ï¼‰**

* `@function_tool`ï¼šå°†æ™®é€š Python å‡½æ•°æ³¨å†Œä¸º Agent å¯è°ƒç”¨çš„å·¥å…·
* `Agent(...)`ï¼šå®šä¹‰ä¸€ä¸ªå…·å¤‡**æŒ‡ä»¤ + æ¨¡å‹ + å·¥å…·**çš„æ™ºèƒ½ä½“
* `instructions`ï¼šç­‰ä»·äº system promptï¼Œç”¨äºçº¦æŸ Agent çš„é£æ ¼ä¸è¡Œä¸º
* `tools`ï¼šç›´æ¥ç»‘å®šå‡½æ•°ï¼Œæ— éœ€æ‰‹åŠ¨å†™ function schema

**Context Configuration**

ä¸‹é¢æ˜¯**å›¾ç‰‡ä¸­ä»£ç çš„å®Œæ•´æ–‡æœ¬ç‰ˆï¼ˆæŒ‰åŸæ ·è¿˜åŸï¼Œå¯ç›´æ¥ç²˜è´´ä½¿ç”¨ï¼‰**ï¼š

```python
from dataclasses import dataclass


@dataclass
class UserContext:
    uid: str
    is_pro_user: bool


async def fetch_purchases() -> list[Purchase]:
    return ...


agent = Agent[UserContext](
    ...
)
```

#### ä»£ç è¦ç‚¹ï¼ˆå¯ç”¨äºåšå®¢è¯´æ˜ï¼‰

* `UserContext`ï¼šä½¿ç”¨ `@dataclass` å®šä¹‰ **Agent Context**ï¼Œç”¨äºåœ¨æ•´ä¸ª agent workflow ä¸­å…±äº«ç”¨æˆ·çŠ¶æ€
* `uid` / `is_pro_user`ï¼šå…¸å‹çš„è·¨å·¥å…·ã€è·¨ handoff å¯å¤ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯
* `Agent[UserContext]`ï¼šé€šè¿‡æ³›å‹æ–¹å¼ï¼Œå°† context ç±»å‹ä¸ Agent ç»‘å®šï¼Œå®ç° **ç±»å‹å®‰å…¨çš„ä¸Šä¸‹æ–‡è®¿é—®**
* `context` åœ¨ Agents SDK ä¸­æ˜¯ **åªè¯»å…±äº«**ï¼Œä¸ä¼šå›  handoff æ”¹å˜ï¼Œéå¸¸é€‚åˆæƒé™ã€ç”¨æˆ·æ€ã€ä¼šè¯ä¿¡æ¯


### Output Configuration

ä¸‹é¢æ˜¯**å›¾ç‰‡ä¸­ä»£ç çš„å®Œæ•´æ–‡æœ¬è¿˜åŸç‰ˆ**ï¼ˆå·²æ•´ç†ä¸ºå¯ç›´æ¥ä½¿ç”¨çš„ Python ä»£ç ï¼‰ï¼š

```python
from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]


agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
```

#### è¿™æ®µä»£ç åœ¨ Agents SDK ä¸­çš„æŠ€æœ¯å«ä¹‰ï¼ˆéå¸¸é€‚åˆå†™è¿›åšå®¢ï¼‰

**1ï¸âƒ£ ä½¿ç”¨ `output_type` å¼ºçº¦æŸ Agent è¾“å‡º**

* `CalendarEvent` ç»§æ‰¿è‡ª `Pydantic BaseModel`
* Agent çš„è¾“å‡º **å¿…é¡»** ç¬¦åˆè¯¥ç»“æ„ï¼Œå¦åˆ™ä¼šè‡ªåŠ¨é‡è¯•æˆ–å¤±è´¥
* è¿™æ˜¯ **é™ä½å¹»è§‰ï¼ˆHallucinationï¼‰** çš„å…³é”®æœºåˆ¶

**2ï¸âƒ£ Agent ä¸å†â€œè‡ªç”±ç”Ÿæˆæ–‡æœ¬â€ï¼Œè€Œæ˜¯â€œç»“æ„åŒ–æŠ½å–â€**

* éå¸¸é€‚åˆï¼š

  * æ—¥ç¨‹ / è¡¨å• / å·¥å• / å®ä½“æŠ½å–
  * ä¸‹æ¸¸ç³»ç»Ÿè‡ªåŠ¨æ¶ˆè´¹ï¼ˆAPIã€æ•°æ®åº“ã€æ¶ˆæ¯é˜Ÿåˆ—ï¼‰

**3ï¸âƒ£ ä¸å¤š Agent ç¼–æ’å¤©ç„¶å…¼å®¹**

* ä¸Šæ¸¸ Agentï¼šè´Ÿè´£ç†è§£è‡ªç„¶è¯­è¨€
* ä¸‹æ¸¸ Agentï¼šåªæ¥æ”¶ `CalendarEvent` ç±»å‹ï¼Œé›¶è§£ææˆæœ¬

> ğŸ“Œ **ä¸€å¥åšå®¢çº§æ€»ç»“**
> é€šè¿‡ `output_type` + Pydanticï¼ŒAgents SDK å°† LLM ä»â€œæ–‡æœ¬ç”Ÿæˆå™¨â€å‡çº§ä¸ºâ€œç±»å‹å®‰å…¨çš„ç»“æ„åŒ–æ•°æ®ç”Ÿäº§è€…â€ã€‚

### Dynamic Instructions


```python
def dynamic_instructions(
    context: RunContextWrapper[UserContext],
    agent: Agent[UserContext],
) -> str:
    return (
        f"The user's name is {context.context.name}. "
        "Help them with their questions."
    )


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
```


#### ğŸ“Œ è¿™æ®µä»£ç åœ¨ Agents SDK ä¸­çš„æ ¸å¿ƒæ„ä¹‰ï¼ˆåšå®¢å¯ç›´æ¥ä½¿ç”¨ï¼‰

**1ï¸âƒ£ Instructions ä¸å†æ˜¯é™æ€å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯å‡½æ•°**

* `instructions` å¯ä»¥æ˜¯ä¸€ä¸ª callable
* åœ¨ **æ¯æ¬¡ Agent è¿è¡Œå‰åŠ¨æ€ç”Ÿæˆ system prompt**
* èƒ½åŸºäºç”¨æˆ·ã€çŠ¶æ€ã€ç¯å¢ƒå®æ—¶è°ƒæ•´è¡Œä¸º

**2ï¸âƒ£ `RunContextWrapper` æ˜¯åŠ¨æ€ä¸Šä¸‹æ–‡çš„å…¥å£**

* `context.context` å³ä½ å®šä¹‰çš„ `UserContext`
* å¯å®‰å…¨è®¿é—®ï¼š

  * ç”¨æˆ·ä¿¡æ¯
  * ä¼šè¯çŠ¶æ€
  * ä¸Šæ¸¸ Agent ä¼ é€’çš„æ•°æ®

**3ï¸âƒ£ éå¸¸é€‚åˆ Triage / Router / Personalization Agent**

å…¸å‹ä½¿ç”¨åœºæ™¯ï¼š

* æ ¹æ®ç”¨æˆ·ç­‰çº§ï¼ˆPro / Freeï¼‰æ”¹å˜å›ç­”ç­–ç•¥
* æ ¹æ®å†å²è¡Œä¸ºå†³å®šæ˜¯å¦å‡çº§ã€è½¬äº¤ã€è°ƒç”¨å·¥å…·
* å¤š Agent ç³»ç»Ÿä¸­çš„ **å…¥å£è°ƒåº¦ Agent**

> ğŸ§  **ä¸€å¥ä¸“ä¸šæ€»ç»“ï¼ˆå¯æ”¾åœ¨åšå®¢æ­£æ–‡ï¼‰**
> Dynamic instructions è®© Agent çš„ system prompt ä»â€œé™æ€é…ç½®â€å‡çº§ä¸ºâ€œè¿è¡Œæ—¶å†³ç­–é€»è¾‘â€ï¼Œè¿™æ˜¯æ„å»ºæ™ºèƒ½åˆ†æµä¸ä¸ªæ€§åŒ– Agent çš„å…³é”®èƒ½åŠ›ã€‚


### Forcing Tool Use

```
ModelSettings (tool_choice-None)
# default â†’ "auto" (LLM decides)

ModelSettings (tool_choice="required" )
# must call a tool

ModelSettings (tool_choice="none")
# no tool allowed

ModelSettings (tool_choice="my_tool")
# must call that specific tool
```

## Orchestrating Multi-step Workflows


åœ¨å®Œæˆå•ä¸ª Agent çš„é…ç½®ä¹‹åï¼ŒOpenAI Agents SDK çš„æ ¸å¿ƒèƒ½åŠ›ä½“ç°åœ¨**å¤š Agent å·¥ä½œæµçš„ç¼–æ’ï¼ˆOrchestrationï¼‰**ä¸Šã€‚SDK æä¾›ä¸‰ç§è¿è¡Œæ–¹å¼ï¼š

* å¼‚æ­¥è¿è¡Œ
* åŒæ­¥è¿è¡Œ
* æµå¼è¿è¡Œ

å…¶ä¸­ **æµå¼è¿è¡Œï¼ˆstreamed runsï¼‰** æ˜¯å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„æ¨èæ–¹å¼ï¼Œå¯æ˜¾è‘—é™ä½ç”¨æˆ·æ„ŸçŸ¥çš„å“åº”å»¶è¿Ÿã€‚

Agent çš„æ‰§è¡Œæœ¬è´¨ä¸Šç”±ä¸€ä¸ª **Agent Loopï¼ˆæ™ºèƒ½ä½“å¾ªç¯ï¼‰** é©±åŠ¨ï¼šRunner ä»¥åˆå§‹ Agent å’Œè¾“å…¥ä¸ºèµ·ç‚¹ï¼Œè°ƒç”¨ LLM ç”Ÿæˆè¾“å‡ºã€‚å¦‚æœè¾“å‡ºå·²ç»æ»¡è¶³ç›®æ ‡ç±»å‹ä¸”ä¸åŒ…å«å·¥å…·è°ƒç”¨æˆ– Agent äº¤æ¥ï¼ˆhandoffï¼‰ï¼Œå¾ªç¯ç«‹å³ç»“æŸï¼›

![Alt Image Text](../images/chap8_0_5.png "Body image")  

**å¦åˆ™ç³»ç»Ÿå°†æ ¹æ®è¾“å‡ºè§¦å‘å·¥å…·è°ƒç”¨æˆ–åˆ‡æ¢åˆ°æ–°çš„ Agentï¼Œå¹¶åœ¨æ›´æ–°ä¸Šä¸‹æ–‡åç»§ç»­ä¸‹ä¸€è½®å¾ªç¯ã€‚ä¸ºé˜²æ­¢æ— é™å¾ªç¯ï¼ŒSDK é€šè¿‡ `max_turns` å‚æ•°å¼ºåˆ¶é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚**

åœ¨å¤š Agent åœºæ™¯ä¸‹ï¼Œå¯¹è¯çŠ¶æ€ç®¡ç†å°¤ä¸ºå…³é”®ã€‚

å¼€å‘è€…æ—¢å¯ä»¥æ‰‹åŠ¨ç»´æŠ¤è¾“å…¥å†å²ï¼ˆå¦‚ä½¿ç”¨ `to_input_list`ï¼‰ï¼Œä¹Ÿå¯ä»¥å€ŸåŠ© **Session æœºåˆ¶**ï¼ˆå¦‚ `SQLiteSession`ï¼‰è‡ªåŠ¨å®Œæˆä¸Šä¸‹æ–‡çš„è¯»å–ä¸æŒä¹…åŒ–ï¼Œä»è€Œç®€åŒ–å¤šè½®å¯¹è¯å’Œå¤šä¼šè¯éš”ç¦»çš„å¤æ‚åº¦ã€‚



åœ¨ç¼–æ’ç­–ç•¥ä¸Šï¼ŒAgents SDK æ”¯æŒä¸¤ç§ä¸»æµæ¨¡å¼ï¼š

* **LLM é©±åŠ¨çš„ç¼–æ’**ï¼šç”±æ¨¡å‹è‡ªä¸»è§„åˆ’æ­¥éª¤ã€é€‰æ‹©å·¥å…·å’Œ Agentï¼Œé€‚åˆé«˜çµæ´»æ€§ä¸å¤æ‚æ¨ç†åœºæ™¯ï¼Œä½†éœ€è¦æ¸…æ™°çš„çº¦æŸã€åé¦ˆæœºåˆ¶ä¸è¯„ä¼°ä½“ç³»ï¼ˆå¦‚ Evalsï¼‰ã€‚


	* Design clear and effective prompts
	* Monitor and enable agent self-improvement
	* Use specialized agents
	* Research evals


> https://platform.openai.com/docs/guides/evals

* **ä»£ç é©±åŠ¨çš„ç¼–æ’**ï¼šé€šè¿‡æ˜¾å¼ä»£ç æ§åˆ¶ Agent çš„æ‰§è¡Œé¡ºåºã€ä¾èµ–å…³ç³»å’Œå¹¶è¡Œæ–¹å¼ï¼Œæ›´å…·ç¡®å®šæ€§ã€å¯é¢„æµ‹æ€§å’Œå·¥ç¨‹å¯æ§æ€§ï¼Œé€‚åˆå¯¹æ€§èƒ½ã€æˆæœ¬å’Œç¨³å®šæ€§è¦æ±‚è¾ƒé«˜çš„ç³»ç»Ÿã€‚


	* Use structured outputs
	* Chain multiple agents
	* Use feedback loops for quality control
	* Speed up with parallel agents


ä¸¤ç§æ–¹å¼å¯ä»¥ç»“åˆä½¿ç”¨ï¼Œä»¥åœ¨æ™ºèƒ½æ€§ä¸å¯æ§æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚


**äºŒã€æŠ€æœ¯æ–‡ç« ä¸“ä¸šè¡¨è¿°ï¼ˆå¯ç›´æ¥å¼•ç”¨ï¼‰**

> **Agents SDK é€šè¿‡å†…ç½®çš„ Agent Loopã€Session ç®¡ç†ä¸çµæ´»çš„ç¼–æ’æœºåˆ¶ï¼Œå°†å¤š Agent åä½œä»â€œæ¦‚å¿µè®¾è®¡â€è½åœ°ä¸ºå¯æ§ã€å¯æ‰©å±•çš„å·¥ç¨‹å®è·µï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåœ¨æ¨¡å‹è‡ªä¸»å†³ç­–ä¸ä»£ç ç¡®å®šæ€§æ§åˆ¶ä¹‹é—´è‡ªç”±æƒè¡¡ï¼Œæ„å»ºé«˜æ€§èƒ½ã€å¯ç»´æŠ¤çš„


### Manage Agent Conversation

å¥½çš„ï¼Œä¸‹é¢æ˜¯**æ ¹æ®ä½ ç»™å‡ºçš„æˆªå›¾å®Œæ•´è¿˜åŸå¹¶æ•´ç†åçš„ä»£ç å†…å®¹**ï¼ˆç›´æ¥å¯å¤åˆ¶ä½¿ç”¨ï¼‰ï¼š

```python
async def main():
    agent = Agent(
        name="QuickMath",
        instructions="Answer with only the final number.",
    )

    thread_id = "thread_1"

    with trace(workflow_name="MathSession", group_id=thread_id):
        # First turn
        result = await Runner.run(
            agent,
            "What is 12 times 8?"
        )
        print(result.final_output)

        # Second turn
        new_input = result.to_input_list() + [
            {
                "role": "user",
                "content": "Now divide it by 4."
            }
        ]

        result = await Runner.run(
            agent,
            new_input
        )
        print(result.final_output)
```

#### ä»£ç è¦ç‚¹è¯´æ˜ï¼ˆç®€è¦ï¼‰

* **`Agent`**ï¼šå®šä¹‰äº†ä¸€ä¸ªåªè¾“å‡ºæœ€ç»ˆæ•°å€¼çš„æ•°å­¦ Agent
* **`trace(...)`**ï¼šç”¨äºå°†ä¸¤æ¬¡è°ƒç”¨å½’å±åˆ°åŒä¸€ä¸ª workflow / threadï¼Œä¾¿äºè°ƒè¯•å’Œå¯è§†åŒ–
* **`result.to_input_list()`**ï¼šæ‰‹åŠ¨æ‹¼æ¥ä¸Šä¸€è½®å¯¹è¯å†å²ï¼Œå®ç°â€œå®¢æˆ·ç«¯ä¾§çš„ä¸Šä¸‹æ–‡å»¶ç»­â€
* **ç¬¬äºŒè½®è°ƒç”¨**ï¼šå¹¶ä¸ä¼šè‡ªåŠ¨è®°å¿†ä¸Šä¸‹æ–‡ï¼Œè€Œæ˜¯æ˜¾å¼æŠŠå†å²ä¼ å› Runner


### Automatically Manage Conversations with Sessions


```python
async def main():
    agent = Agent(
        name="AnimalHelper",
        instructions="Give short and clear answers about animals.",
    )

    # Create session instance
    session = SQLiteSession("animal_chat_001")

    with trace(
        workflow_name="AnimalTalk",
        group_id="animal_group_001"
    ):
        # First turn
        result = await Runner.run(
            agent,
            "What animal is the biggest in the ocean?",
            session=session
        )
        print(result.final_output)

        # Second turn
        result = await Runner.run(
            agent,
            "How much does it weigh?",
            session=session
        )
        print(result.final_output)
```

---

#### è¿™ä¸ªç¤ºä¾‹åœ¨åšå®¢é‡Œå¯ä»¥å¼ºè°ƒçš„å‡ ä¸ªå…³é”®ç‚¹

* **`SQLiteSession`**
  ç”¨äº**è‡ªåŠ¨æŒä¹…åŒ–å¯¹è¯å†å²**ï¼Œæ— éœ€æ‰‹åŠ¨æ‹¼æ¥ `to_input_list()`

* **åŒä¸€ä¸ª `session`**
  ç¬¬äºŒè½®é—®é¢˜ *â€œHow much does it weigh?â€* èƒ½æ­£ç¡®ç†è§£ä¸Šä¸‹æ–‡ï¼ˆæŒ‡è“é²¸ï¼‰

* **ä¸å‰ä¸€ä¸ªç¤ºä¾‹çš„å¯¹æ¯”**

  * âŒ æ‰‹åŠ¨ç®¡ç†ï¼š`result.to_input_list()`
  * âœ… è‡ªåŠ¨ç®¡ç†ï¼š`SQLiteSession`

ä¸€å¥éå¸¸é€‚åˆæ”¾åœ¨åšå®¢é‡Œçš„æ€»ç»“æ˜¯ï¼š

> **Session å°† Agent ä»â€œæ— çŠ¶æ€å‡½æ•°è°ƒç”¨â€å‡çº§ä¸ºâ€œæœ‰è®°å¿†çš„å¯¹è¯ç³»ç»Ÿâ€ï¼Œæ˜¯æ„å»ºçœŸå®åº”ç”¨çš„å…³é”®èƒ½åŠ›ã€‚**



